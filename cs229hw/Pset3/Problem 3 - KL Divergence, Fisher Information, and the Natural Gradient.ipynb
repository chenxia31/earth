{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. KL Divergence, Fisher Information, and the Natural Gradient\n",
    "\n",
    "\n",
    "As seen before, the Kullback-Leibler divergence between two distributions is an asymmetric measure of how different two distributions are. Consider two distributions over the same space given by densities $p(x)$ and $q(x)$. The KL divergence between two continuous distributions, $q$ and $p$ is defined as,\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(p\\|q) \n",
    "& = \\int_{-\\infty}^\\infty p(x)\\log\\frac{p(x)}{q(x)}dx\\\\\n",
    "& = \\int_{-\\infty}^\\infty p(x)\\log p(x)dx - \\int_{-\\infty}^\\infty p(x)\\log q(x)dx\\\\\n",
    "& = \\mathbb{E}_{x\\sim p(x)}[\\log p(x)] - \\mathbb{E}_{x\\sim p(x)}[\\log(q(x)].\n",
    "\\end{align*}\n",
    "\n",
    "A nice property of KL divergence is that it invariant to parametrization. This means, KL divergence evaluates to the same value no matter how we parametrize the distributions $P$ and $Q$. For e.g, if $P$ and $Q$ are in the exponential family, the KL divergence between them is the same whether we are using natural parameters, or canonical parameters, or any arbitrary reparametrization.\n",
    "\n",
    "Now we consider the problem of fitting model parameters using gradient descent (or stochastic gradient descent). As seen previously, fitting model parameters using Maximum Likelihood is equivalent to minimizing the KL divergence between the data and the model. While KL divergence is invariant to parametrization, the gradient w.r.t the model parameters (i.e, direction of steepest descent) is not invariant to parametrization. To see its implication, suppose we are at a particular value of parameters (either randomly initialized, or mid-way through the optimization process). The value of the parameters correspond to some probability distribution (and in case of regression, a conditional probability distribution). If we follow the direction of steepest descent from the current parameter, take a small step along that direction to a new parameter, we end up with a new distribution corresponding to the new parameters. The non- invariance to reparametrization means, a step of fixed size in the parameter space could end up in a distribution that could either be extremely far away in $D_{KL}$ from the previous distribution, or on the other hand not move very much at all w.r.t $D_{KL}$ from the previous distributions.\n",
    "\n",
    "This is where the natural gradient comes into picture. It is best introduced in contrast with the usual gradient descent. In the usual gradient descent, we first choose the direction by calculating the gradient of the MLE objective w.r.t the parameters, and then move a magnitude of step size (where size is measured in the parameter space) along that direction. Whereas in natural gradi- ent, we first choose a divergence amount by which we would like to move, in the $D_{KL}$ sense. This effectively gives us a perimeter around the current parameters (of some arbitrary shape), such that points along this perimeter correspond to distributions which are at an equal $D_{KL}$-distance away from the current parameter. Among the set of all distributions along this perimeter, we move to the distribution that maximizes the objective (i.e minimize $D_{KL}$ between data and itself) the most. This approach makes the optimization process invariant to parametrization. That means, even if we chose a new arbitrary reparametrization, by starting from a particular distribution, we always descend down the same sequence of distributions towards the optimum.\n",
    "\n",
    "In the rest of this problem, we will construct and derive the natural gradient update rule. For that, we will break down the process into smaller sub-problems, and give you hints to answer them. Along the way, we will encounter important statistical concepts such as the score function and Fisher Information (which play a prominant role in Statistical Learning Theory as well). Finally, we will see how this new natural gradient based optimization is actually equivalent to Newton’s method for Generalized Linear Models.\n",
    "\n",
    "Let the distribution of a random variable $Y$ parameterized by $\\theta\\in\\mathbb{R}^n$ be $p(y;\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [3 points] Score function\n",
    "\n",
    "\n",
    "The score function associated with $p(y;\\theta)$ is defined as $\\nabla_\\theta \\log p(y;\\theta)$, which signifies the sensitivity of the likelihood function with respect to the parameters. Note that the score\n",
    "function is actually a vector since it’s the gradient of a scalar quantity with respect to the\n",
    "vector $\\theta$.\n",
    "\n",
    "Recall that $\\mathbb{E}_{x\\sim p(y)}[g(y)] =\\int_{-\\infty}^\\infty g(y)p(y)dy $. Using this fact, show that the expected value of the score is $)$, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\nabla_{\\theta'} \\log p(y;\\theta')|_{\\theta'=\\theta}\\right] = 0.\n",
    "\\end{align*}\n",
    "\n",
    "### Asnwer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\nabla_{\\theta'} \\log p(y;\\theta')|_{\\theta'=\\theta}\\right]\n",
    "& = \\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\frac{1}{p(y;\\theta)}\\frac{\\partial p(y;\\theta)}{\\partial \\theta}\\right]\\\\\n",
    "& = \\int_{-\\infty}^\\infty p(y;\\theta) \\frac{1}{p(y;\\theta)}\\frac{\\partial p(y;\\theta)}{\\partial \\theta} dy\\\\\n",
    "& = \\int_{-\\infty}^\\infty \\frac{\\partial p(y;\\theta)}{\\partial \\theta} dy\\\\\n",
    "& = \\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^\\infty  p(y;\\theta) dy\\\\\n",
    "& = \\frac{\\partial}{\\partial \\theta} 1 \\\\\n",
    "& = 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [2 points] Fisher Information\n",
    "\n",
    "Let us now introduce a quantity known as the Fisher information. It is defined as the covariance matrix of the score function,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{I}(\\theta) = \n",
    "{\\rm Cov}_{x\\sim p(y;\\theta)}\\left[\\nabla_{\\theta'} \\log p(y;\\theta')|_{\\theta'=\\theta}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Intuitively, the Fisher information represents the amount of information that a random variable $Y$ carries about a parameter $\\theta$ of interest. When the parameter of interest is a vector (as in our case, since $\\theta\\in\\mathbb{R}^n$), this information becomes a matrix. Show that the Fisher information can equivalently be given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{I}(\\theta) =\n",
    "\\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\nabla_{\\theta'} \\log p(y;\\theta')\\nabla_{\\theta'} \\log p(y;\\theta')^T|_{\\theta'=\\theta}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Note that the Fisher Information is a function of the parameter. The parameter of the Fisher information is both a) the parameter value at which the score function is evaluated, and b) the parameter of the distribution with respect to which the expectation and variance is calculated.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that\n",
    "\\begin{align*}\n",
    "{\\rm Cov}(X) = \\mathbb{E}\\left[\\left(X-\\mathbb{E}(X)\\right)\\left(X-\\mathbb{E}(X)\\right)^T\\right]\n",
    "\\end{align*}\n",
    "and, hence, when $\\mathbb{E}(X) = 0$\n",
    "\\begin{align*}\n",
    "{\\rm Cov}(X) = \\mathbb{E}\\left[XX^T\\right].\n",
    "\\end{align*}\n",
    "Consequently, \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{I}(\\theta) \n",
    "& = {\\rm Cov}_{x\\sim p(y;\\theta)}\\left[\\nabla_{\\theta'} \\log p(y;\\theta')|_{\\theta'=\\theta}\\right] \\\\\n",
    "& = \\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\nabla_{\\theta'} \\log p(y;\\theta')\\nabla_{\\theta'} \\log p(y;\\theta')^T|_{\\theta'=\\theta}\\right].\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [5 points] Fisher Information (alternate form)\n",
    "\n",
    "It turns out that the Fisher Information can not only be defined as the covariance of the\n",
    "score function, but in most situations it can also be represented as the expected negative Hessian of the log-likelihood.\n",
    "\n",
    "Show that\n",
    "\\begin{align*}\n",
    "\\mathcal{I}(\\theta) = \\mathbb{E}_{x\\sim p(y;\\theta)}\\left[-\\nabla^2_{\\theta'} \\log p(y;\\theta')|_{\\theta'=\\theta}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "**Remark.** The Hessian represents the curvature of a function at a point. This shows that the expected curvature of the log-likelihood function is also equal to the Fisher information matrix. If the curvature of the log-likelihood at a parameter is very steep (i.e, Fisher Information is very high), this generally means you need fewer number of data samples to a estimate that parameter well (assuming data was generated from the distribution with those parameters), and vice versa. The Fisher information matrix associated with a statistical model parameterized by $\\theta$ is extremely important in determining how a model behaves as a function of the number of training set examples.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta'} \\log p(y;\\theta') \n",
    "& = \\frac{1}{p(y;\\theta')}\\nabla_{\\theta'} p(y;\\theta')\n",
    "\\end{align*}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla^2_{\\theta'} \\log p(y;\\theta') \n",
    "& = \\frac{\\partial}{\\partial \\theta'}\\Big(\\frac{1}{p(y;\\theta')}\\nabla_{\\theta'} p(y;\\theta')\\Big)\\\\\n",
    "& = \\frac{1}{p(y;\\theta')^2}\\nabla_{\\theta'} p(y;\\theta')\\nabla_{\\theta'} p(y;\\theta')^T + \\frac{1}{p(y;\\theta')}\\nabla^2_{\\theta'} p(y;\\theta').\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\nabla^2_{\\theta'} \\log p(y;\\theta')|_{\\theta'=\\theta}\\right]\n",
    "& = \\mathbb{E}_{x\\sim p(y;\\theta')}\\left[-\\frac{1}{p(y;\\theta')^2}\\nabla_{\\theta'} p(y;\\theta')\\nabla_{\\theta'} p(y;\\theta')^T + \\frac{1}{p(y;\\theta')}\\nabla^2_{\\theta'} p(y;\\theta')|_{\\theta =\\theta'}\\right]\\\\\n",
    "& = -\\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\nabla_{\\theta'} \\log p(y;\\theta')\\nabla_{\\theta'} \\log p(y;\\theta')^T|_{\\theta'=\\theta}\\right] +\n",
    "\\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\frac{1}{p(y;\\theta')}\\nabla^2_{\\theta'} p(y;\\theta')|_{\\theta =\\theta'}\\right]\\\\\n",
    "& = -\\mathcal{I}(\\theta) + 0\n",
    "\\end{align*}\n",
    "\n",
    "For the last equality, we used the follwoing fact.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{x\\sim p(y;\\theta)}\\left[\\frac{1}{p(y;\\theta')}\\nabla^2_{\\theta'} p(y;\\theta')|_{\\theta =\\theta'}\\right]\n",
    "& = \\int_{-\\infty}^{\\infty} p(y;\\theta)\\frac{1}{p(y;\\theta)}\\nabla^2_{\\theta} p(y;\\theta)dy\\\\\n",
    "& = \\int_{-\\infty}^{\\infty} \\nabla^2_{\\theta} p(y;\\theta)dy\\\\\n",
    "& = \\nabla^2_{\\theta}\\int_{-\\infty}^{\\infty} p(y;\\theta)dy\\\\\n",
    "& = \\nabla^2_{\\theta} 1 \\\\\n",
    "& = 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [5 points] Approximating $D_{KL}$ with Fisher Information\n",
    "\n",
    "As we explained at the start of this problem, we are interested in the set of all distributions that are at a small fixed $D_{KL}$ distance away from the current distribution. In order to calculate DKL between $p(y;\\theta)$ and $p(y;\\theta+d)$, where $d\\in\\mathbb{R}^n$ is a small magnitude “delta” vector, we approximate it using the Fisher Information at $\\theta$. Eventually $d$ will be the natural gradient update we will add to $\\theta$. To approximate the KL-divergence with FisherInfomration, we will start with the Taylor Series expansion of $D_{KL}$ and see that the Fisher Information pops up in the expansion.\n",
    "\n",
    "Show that $D_{KL}(p_{\\theta}\\|p_{\\theta+d})\\approx \\frac{1}{2}d^T\\mathcal{I}(\\theta)d.$\n",
    "\n",
    "**Hint:** Start with the Taylor Series expansion of $D_{KL}(p_{\\theta}\\|p_{\\tilde{\\theta}})$ where $\\theta$ is a constant and $\\tilde{\\theta}$ is a variable. Later set $\\tilde{\\theta} = \\theta+d$. Recall that the Taylor Series allows us to approximate a scalar function $f(\\tilde{\\theta})$ near $\\theta$ by:\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\tilde{\\theta}) \\approx f(\\theta) + (\\tilde{\\theta} - \\theta)^T\\nabla_{\\theta'}f(\\theta')|_{\\theta' = \\theta}\n",
    "+ \\frac{1}{2}(\\tilde{\\theta} - \\theta)^T\\big(\\nabla^2_{\\theta'}f(\\theta')|_{\\theta' = \\theta}\\big)(\\tilde{\\theta} - \\theta).\n",
    "\\end{align*}\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set \n",
    "\\begin{align*}\n",
    "f(\\tilde{\\theta}) &= \\mathbb{E}_{y\\sim p_{\\theta}(y)}\\left[D_{KL}(p_{\\theta}(y)\\|p_{\\tilde{\\theta}}(y))\\right]\\\\\n",
    "& = \\mathbb{E}_{y\\sim p_{\\theta}(y)}\\left[\\log p_{\\theta}(y)\\right] - \\mathbb{E}_{y\\sim p_{\\theta}(y)}\\left[\\log p_{\\tilde{\\theta}}(y))\\right].\n",
    "\\end{align*}\n",
    "\n",
    "First, note that $f(\\theta) = 0$.\n",
    "Using Parts (a) and (c), we have \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\tilde{\\theta}} f(\\tilde{\\theta})|_{\\tilde{\\theta}=\\theta} \n",
    "& = - \\mathbb{E}_{y\\sim p_{\\theta}(y)}\\left[\\nabla_{\\tilde{\\theta}}\\log p_{\\tilde{\\theta}}(y)|_{\\tilde{\\theta}=\\theta}\\right]\\\\\n",
    "& = 0\n",
    "\\end{align*}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla^2_{\\tilde{\\theta}} f(\\tilde{\\theta})|_{\\tilde{\\theta}=\\theta} \n",
    "& = - \\mathbb{E}_{y\\sim p_{\\theta}(y)}\\left[\\nabla^2_{\\tilde{\\theta}}\\log p_{\\tilde{\\theta}}(y)|_{\\tilde{\\theta}=\\theta}\\right]\\\\\n",
    "& = \\mathcal{I}(\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "Remind that $\\tilde{\\theta} = \\theta +d$. Using the Taylor Series to approximate the scalar function $f(\\tilde{\\theta})$ near $\\theta$, we have\n",
    "\\begin{align*}\n",
    "f(\\tilde{\\theta}) \n",
    "& \\approx f(\\theta) + (\\tilde{\\theta} - \\theta)^T\\nabla_{\\theta'}f(\\theta')|_{\\theta' = \\theta}\n",
    "+ \\frac{1}{2}(\\tilde{\\theta} - \\theta)^T\\big(\\nabla^2_{\\theta'}f(\\theta')|_{\\theta' = \\theta}\\big)(\\tilde{\\theta} - \\theta)\\\\\n",
    " & = \\frac{1}{2}d^T\\mathcal{I}(\\theta)d.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) [8 points] Natural Gradient\n",
    "\n",
    "Now we move on to calculating the natural gradient. Recall that we want to maximize the log-likelihood by moving only by a fixed $D_{KL}$ distance from the current position. In the previous sub-question we came up with a way to approximate DKL distance with Fisher Information. Now we will set up the constrained optimization problem that will yield the natural gradient update d. Let the log-likelihood objective be $\\ell(\\theta)= \\log p(y,\\theta)$. Let the $D_{KL}$ distance we want to move by, be some small positive constant $c$. The natural gradient update $d^*$ is\n",
    "\n",
    "\\begin{equation*}\n",
    "d^* = \\arg\\max_{d}\\ell(\\theta+d) \\quad  \\, subject\\, to\\,\\quad D_{KL}(p_\\theta\\|p_{\\theta+d}) = c.\n",
    "\\label{1} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "First we note that we can use Taylor approximation on $\\ell(\\theta+d)\\approx \\ell(\\theta) +d^T\\nabla_{\\theta'}\\ell(\\theta')|_{\\theta' =\\theta}$. Also note that we calculated the Taylor approximation $D_{KL}(p_{\\theta}\\|p_{\\theta+d})$ in the previous sub-problem. We shall substitute both these approximations into the above constrainted optimization problem.\n",
    "\n",
    "In order to solve this constrained optimization problem, we employ the method of Lagrange multipliers. If you are familiar with Lagrange multipliers, you can proceed directly to solve for $d^*$. If you are not familiar with Lagrange multipliers, here is a simplified introduction. (You may also refer to a slightly more comprehensive introduction in the Convex Optimization section notes, but for the purposes of this problem, the simplified introduction provided here should suffice).\n",
    "\n",
    "Consider the following constrained optimization problem\n",
    "\\begin{align*}\n",
    "d^* = \\arg\\max_{d}f(d) \\quad  \\, subject\\, to\\,\\quad g(d) = c.\n",
    "\\end{align*}\n",
    "The function $f$ is the objective function and $g$ is the constraint. We instead optimize the Lagrangian $\\mathcal{L}(d,\\lambda)$, which is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(d,\\lambda) = f(d)- \\lambda[g(d)-c].\n",
    "\\end{align*}\n",
    "with respect to both d and $\\lambda$. Here $\\lambda\\in \\mathbb{R}_+$ is called the Lagrange multiplier. In order to\n",
    "optimize the above, we construct the following system of equations:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_d\\mathcal{L}(d,\\lambda)=0,\n",
    "\\label{a} \\tag{a}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_\\lambda\\mathcal{L}(d,\\lambda)=0\n",
    "\\label{b} \\tag{b}\n",
    "\\end{equation*}\n",
    "\n",
    "So we have two equations (a and b above) with two unknowns ($d$ and $\\lambda$), which can be sometimes be solved analytically (in our case, we can).\n",
    "\n",
    "The following steps guide you through solving the constrained optimization problem:\n",
    "\n",
    "- Construct the Lagrangian for the constrained optimization problem (1) with the Taylor approximations substituted in for both the objective and the constraint.\n",
    "\n",
    "\n",
    "- Then construct the system of linear equations (like (a) and (b)) from the Lagrangian you obtained.\n",
    "\n",
    "\n",
    "- From (a), come up with an expression for $d$ that involves $\\lambda$. At this stage we have already found the \"direction\" of the natural gradient d, since $\\lambda$ is only a positive scaling constant. For most practical purposes, the solution we obtain here is sufficient. This is because we almost always include a learning rate hyperparameter in our optimization algorithms, or perform some kind of a line search for algorithmic stability. This can make the exact calculation of $\\lambda$ less critical. Let's call this expression $\\tilde{d}$ (involving $\\lambda$) as the unscaled natural gradient. Clearly state what is d ̃ as a function of $\\lambda$. \n",
    "The remaining steps are to figure out the value of the scaling constant $\\lambda$ along the direction of $d$, for completeness.\n",
    "\n",
    "\n",
    "- Plugin that expression for $d$ into (b). Now we have an equation that has $\\lambda$ but not $d$. Come up with an expression for $\\lambda$ that does not include $d$.\n",
    "\n",
    "\n",
    "- Plug that expression for $\\lambda$ (without $d$) back into (a). Now we have an equation that has $d$ but not $\\lambda$. Come up with an expression for $d$ that does not include $\\lambda$.\n",
    "\n",
    "The expression for $d$ obtained this way will be the desired natural gradient update $d^*$. Clearly state and highlight your final expression for $d^*$. This expression cannot include $\\lambda$.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remind that $\\ell(\\theta)= \\log p(y,\\theta)$ and hence \n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta'}\\ell(\\theta')|_{\\theta' =\\theta} = \\frac{1}{p(y,\\theta)}\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}.\n",
    "\\end{align*} \n",
    "Using Taylor approximation, we can rewrite (1) as follows:\n",
    "\\begin{equation*}\n",
    "d^* = \\arg\\max_{d}  \\ell(\\theta) +d^T\\nabla_{\\theta'}\\ell(\\theta')|_{\\theta' =\\theta} \\quad  \\, subject\\, to\\,\\quad \\frac{1}{2}d^T\\mathcal{I}(\\theta)d = c.\n",
    "\\label{a} \\tag{a}\n",
    "\\end{equation*}\n",
    "\n",
    "Since $\\ell(\\theta)$ and $\\frac{1}{p(y,\\theta)}$ are positive constants w.r.t. $d$, this could be simplified even more to  \n",
    "\n",
    "\\begin{equation*}\n",
    "d^* = \\arg\\max_{d} d^T\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta} \\quad  \\, subject\\, to\\,\\quad \\frac{1}{2}d^T\\mathcal{I}(\\theta)d = c.\n",
    "\\label{b} \\tag{b}.\n",
    "\\end{equation*}\n",
    "\n",
    "The Lagrangian $\\mathcal{L}(d,\\lambda)$ for the constrained optimization problem (b) is \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(d,\\lambda)& = d^T\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta} - \\lambda(\\frac{1}{2}d^T\\mathcal{I}(\\theta)d - c).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can insted optimize $\\mathcal{L}(d,\\lambda)$ w.r.t both $d$ and $\\lambda$, where $\\lambda\\in\\mathbb{R}_+$. To optimize $\\mathcal{L}(d,\\lambda)$, we construct the following system of equations:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_d\\mathcal{L}(d,\\lambda) = \\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta} - \\lambda\\mathcal{I}(\\theta)d = 0,\n",
    "\\label{i} \\tag{i}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_\\lambda\\mathcal{L}(d,\\lambda) = -\\frac{1}{2}d^T\\mathcal{I}(\\theta)d + c = 0\n",
    "\\label{ii} \\tag{ii}\n",
    "\\end{equation*}\n",
    "\n",
    "From (i), we obtain the unscaled natural gradient $\\tilde{d}(\\lambda)$ as follwos,   \n",
    "\n",
    "\\begin{equation*}\n",
    "\\tilde{d} = \\frac{1}{\\lambda}\\mathcal{I}(\\theta)^{-1}\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}.\n",
    "\\end{equation*}\n",
    "\n",
    "Pluging this expression into (ii), we have \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{2\\lambda^2}\\left(\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}\\right)^T\\mathcal{I}(\\theta)^{-1}\\mathcal{I}(\\theta)\n",
    "\\mathcal{I}(\\theta)^{-1}\\left(\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}\\right)^T\n",
    "& = \\frac{1}{2\\lambda^2}\\left(\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}\\right)^T\n",
    "\\mathcal{I}(\\theta)^{-1}\\left(\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}\\right)\\\\\n",
    "& = c.\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda = \\sqrt{\\frac{1}{2c}\\left(\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}\\right)^T\n",
    "\\mathcal{I}(\\theta)^{-1}\\left(\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}\\right)}.\n",
    "\\end{align*}\n",
    "\n",
    "Plug it into the formula of $\\tilde{d}$, we obtain \n",
    "\n",
    "\\begin{align*}\n",
    "d^* =  \\sqrt{\\frac{2c}{\\left(\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}\\right)^T\n",
    "\\mathcal{I}(\\theta)^{-1}\\left(\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}\\right)}}\\mathcal{I}(\\theta)^{-1}\\nabla_{\\theta'}p(y,\\theta)|_{\\theta'=\\theta}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) [2 points] Relation to Newton’s Method\n",
    "\n",
    "After going through all these steps to calculate the natural gradient, you might wonder if this is something used in practice. We will now see that the familiar Newton's method that we studied earlier, when applied to Generalized Linear Models, is equivalent to natural gradient on Generalized Linear Models. While the two methods (Netwon's and natural gradient) agree on GLMs, in general they need not be equivalent.\n",
    "\n",
    "Show that the direction of update of Newton's method, and the direction of natural gradient, are exactly the same for Generalized Linear Models. You may want to recall and cite the results you derived in problem set 1 question 4 (Convexity of GLMs). For the natural gradient, it is sufficient to use $\\tilde{d}$, the unscaled natural gradient.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In view of Part (e), to maximize $\\ell(\\theta)=\\log p(y;\\theta)$ (MLE), the update rule based on natural gradient method is \n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_{new} := \\theta_{old} + \\alpha\\mathcal{I}(\\theta_{old})^{-1}\\nabla_{\\theta}p(y,\\theta).\n",
    "\\label{i} \\tag{i}\n",
    "\\end{equation*}\n",
    "\n",
    "To recap, an exponential family distribution is one whose probability density can be represented\n",
    "$$p(y,\\eta)=b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)$$\n",
    "where $\\eta$ is the natural parameter of the distribution. Moreover, in a Generalized Linear Model, $\\eta$ is modeled as $\\theta^Tx$, where $x\\in \\mathbb{R}^n$ is the input features of the example, and $\\theta\\in\\mathbb{R}^n$ is learnable parameters. \n",
    "\n",
    "\n",
    "In view of problem set 1 question 4 (Convexity of GLMs) Part (c), the second derivative (i.e., Hessian) of the function $\\ell(\\theta) = \\log p(y;\\theta)$, $H(\\theta)$, w.r.t the model parameter $\\theta$ is a constant function w.r.t. $y$ which implies   \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{I}(\\theta) \n",
    "& = \\mathbb{E}_{x\\sim p(y;\\theta)}\\left[-\\nabla^2_{\\theta'} \\log p(y;\\theta')|_{\\theta'=\\theta}\\right]\\\\\n",
    "& = \\mathbb{E}_{x\\sim p(y;\\theta)}\\left[-H(\\theta)\\right]\\\\\n",
    "& = -H(\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "Plug it into Equation (i), the natural gradient update rule for Generalized Linear Model would be \n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{new} := \\theta_{old} - \\alpha H(\\theta_{old})^{-1}\\nabla_{\\theta_{old}}p(y,\\theta_{old}).\n",
    "\\end{align*}\n",
    "\n",
    "Newton's method suggests the follwoing update rule to maximize $l(\\theta)$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{new} := \\theta_{old} - H(\\theta_{old})^{-1}\\nabla_{\\theta_{old}}p(y,\\theta_{old}).\n",
    "\\end{align*}\n",
    "\n",
    "Note that $l(\\theta)$ is a negative semi-definite function. Therefore,  Newton's method convereges to its global maximum. \n",
    "\n",
    "Comparing these two latter update rules, it is clear that the direction of update of Newton's method and the direction of natural gradient are exactly the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
