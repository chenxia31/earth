{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Spam classification\n",
    "\n",
    "In this problem, we will use the naive Bayes algorithm and an SVM to build a spam classifier.\n",
    "In recent years, spam on electronic media has been a growing concern. Here, we'll build a classifier to distinguish between real messages, and spam messages. For this class, we will be building a classifier to detect SMS spam messages. We will be using an SMS spam dataset developed by Tiago A. Almedia and Jose Marıa Gomez Hidalgo which is publicly available on http://www.dt.fee.unicamp.br/~tiago/smsspamcollection.\n",
    "\n",
    "We have split this dataset into training and testing sets and have included them in this assignment as `data/ds6_spam_train.tsv` and `data/ds6_spam_test.tsv`. See `data/ds6_readme.txt` for more details about this dataset. Please refrain from redistributing these dataset files. The goal of this assignment is to build a classifier from scratch that can tell the difference the spam and non-spam messages using the text of the SMS message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ [5 points] Implement code for processing the the spam messages into numpy arrays that can\n",
    "be fed into machine learning models. Do this by completing the `get_words`, `create_dictionary`, and `transform_text` functions within our provided `src/p06_spam.py`. Do note the corresponding comments for each function for instructions on what specific processing is required. The provided code will then run your functions and save the resulting dictionary into `output/p06_dictionary` and a sample of the resulting training matrix into\n",
    "`output/p06_sample_train_matrix`.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import src.util as util\n",
    "import src.svm as svm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(message):\n",
    "    \"\"\"Get the normalized list of words from a message string.\n",
    "\n",
    "    This function should split a message into words, normalize them, and return\n",
    "    the resulting list. For splitting, you should split on spaces. For normalization,\n",
    "    you should convert everything to lowercase.\n",
    "\n",
    "    Args:\n",
    "        message: A string containing an SMS message\n",
    "\n",
    "    Returns:\n",
    "       The list of normalized words from the message.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    return message.lower().split()\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "def create_dictionary(messages):\n",
    "    \"\"\"Create a dictionary mapping words to integer indices.\n",
    "\n",
    "    This function should create a dictionary of word to indices using the provided\n",
    "    training messages. Use get_words to process each message. \n",
    "\n",
    "    Rare words are often not useful for modeling. Please only add words to the dictionary\n",
    "    if they occur in at least five messages.\n",
    "\n",
    "    Args:\n",
    "        messages: A list of strings containing SMS messages\n",
    "\n",
    "    Returns:\n",
    "        A python dict mapping words to integers.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    all_words = [word for message in messages for word in get_words(message)]\n",
    "    word_counts = collections.Counter(all_words)\n",
    "    words = [word for word in word_counts if word_counts[word]>=5]\n",
    "    return {words[ind]: ind for ind in range(len(words)) }\n",
    "\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "def transform_text(messages, word_dictionary):\n",
    "    \"\"\"Transform a list of text messages into a numpy array for further processing.\n",
    "\n",
    "    This function should create a numpy array that contains the number of times each word\n",
    "    appears in each message. Each row in the resulting array should correspond to each \n",
    "    message and each column should correspond to a word.\n",
    "\n",
    "    Use the provided word dictionary to map words to column indices. Ignore words that \n",
    "    are not present in the dictionary. Use get_words to get the words for a message.\n",
    "\n",
    "    Args:\n",
    "        messages: A list of strings where each string is an SMS message.\n",
    "        word_dictionary: A python dict mapping words to integers.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array marking the words present in each message.\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    m, n = len(messages), len(word_dictionary)\n",
    "    array = np.zeros((m,n), dtype=int)\n",
    "    \n",
    "    #list_words = list(word_dictionary.keys())\n",
    "    \n",
    "    for i in range(m):\n",
    "        words_count = collections.Counter(get_words(messages[i]))\n",
    "        for word in  words_count:\n",
    "            if word in word_dictionary:\n",
    "                array[i,word_dictionary[word]] = words_count[word]   \n",
    "    return array\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ [10 points] In this question you are going to implement a naive Bayes classifier for spam classification with multinomial event model and Laplace smoothing (refer to class notes on Naive Bayes for details on Laplace smoothing).\n",
    "\n",
    "Write your implementation by completing the fit naive bayes model and\n",
    "predict from naive bayes model functions in `src/p06_spam.py`.\n",
    "`src/p06_spam.py` should then be able to train a Naive Bayes model, compute your prediction accuracy and then save your resulting predictions to `output/p06_naive_bayes_predictions`. Remark. If you implement naive Bayes the straightforward way, you'll find that the computed $p(x|y)= \\prod_i p(x_i|y)$ often equals zero. This is because $p(x|y)$, which is the product of many numbers less than one, is a very small number. The standard computer representation of real numbers cannot handle numbers that are too small, and instead rounds them off to zero. (This is called “underflow.”) You’ll have to find a way to compute Naive Bayes’ predicted class labels without explicitly representing very small numbers such\n",
    "as $p(x|y)$. \n",
    "\n",
    "__Hint:__ Think about using logarithms.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes_model(matrix, labels):\n",
    "    \"\"\"Fit a naive bayes model.\n",
    "\n",
    "    This function should fit a Naive Bayes model given a training matrix and labels.\n",
    "\n",
    "    The function should return the state of that model.\n",
    "\n",
    "    Feel free to use whatever datatype you wish for the state of the model.\n",
    "\n",
    "    Args:\n",
    "        matrix: A numpy array containing word counts for the training data\n",
    "        labels: The binary (0 or 1) labels for that training data\n",
    "\n",
    "    Returns: The trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    m, n = matrix.shape\n",
    "    phi_y = np.mean(labels)\n",
    "    phi_k_y1 = (matrix[labels==1].sum(axis = 0) +1)/(matrix[labels==1].sum()+n)\n",
    "    phi_k_y0 = (matrix[labels==0].sum(axis = 0) +1)/(matrix[labels==0].sum()+n)\n",
    "    return phi_y, phi_k_y0, phi_k_y1\n",
    "\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_naive_bayes_model(model, matrix):\n",
    "    \"\"\"Use a Naive Bayes model to compute predictions for a target matrix.\n",
    "\n",
    "    This function should be able to predict on the models that fit_naive_bayes_model\n",
    "    outputs.\n",
    "\n",
    "    Args:\n",
    "        model: A trained model from fit_naive_bayes_model\n",
    "        matrix: A numpy array containing word counts\n",
    "\n",
    "    Returns: A numpy array containg the predictions from the model\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    phi_y, phi_k_y0, phi_k_y1 = model\n",
    "    log_p_y_1 = matrix @ np.log(phi_k_y1) + np.log(phi_y)\n",
    "    log_p_y_0 = matrix @ np.log(phi_k_y0) + np.log(1-phi_y)\n",
    "    return (log_p_y_1 >= log_p_y_0)\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ [5 points] Intuitively, some tokens may be particularly indicative of an SMS being in a particular class. We can try to get an informal sense of how indicative token $i$ is for the SPAM class by looking at:\n",
    "\n",
    "\\begin{align*}\n",
    "\\log\\frac{P(x_j=i|y=1)}{P(x_j=i|y=0)} = \\log\\frac{P(\\mbox{token $i|$ email is SPAM)}}{P (\\mbox{token $i|$email is NOTSPAM)}}.\n",
    "\\end{align*}\n",
    "\n",
    "Complete the `get_top_five_naive_bayes_words` function within the provided code using the above formula in order to obtain the $5$ most indicative tokens.\n",
    "\n",
    "The provided code will print out the resulting indicative tokens and then save thm to\n",
    "`output/p06_top_indicative_words`.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_five_naive_bayes_words(model, dictionary):\n",
    "    \"\"\"Compute the top five words that are most indicative of the spam (i.e positive) class.\n",
    "\n",
    "    Ues the metric given in 6c as a measure of how indicative a word is.\n",
    "    Return the words in sorted form, with the most indicative word first.\n",
    "\n",
    "    Args:\n",
    "        model: The Naive Bayes model returned from fit_naive_bayes_model\n",
    "        dictionary: A mapping of word to integer ids\n",
    "\n",
    "    Returns: The top five most indicative words in sorted order with the most indicative first\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    phi_y, phi_k_y0, phi_k_y1 = model\n",
    "    top_5_index = np.argsort(-np.log(phi_k_y1) + np.log(phi_k_y0))[:5]\n",
    "    reverced_dictionary = {dictionary[k]:k for k in dictionary}\n",
    "    return [reverced_dictionary[i] for i in top_5_index]\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ [2 points] Support vector machines (SVMs) are an alternative machine learning model that we discussed in class. We have provided you an SVM implementation (using a radial basis function (RBF) kernel) within `src/svm.py` (You should not need to modify that code). One important part of training an SVM parameterized by an RBF kernel is choosing an appropriate kernel radius.\n",
    "\n",
    "Complete the `compute_best_svm_radius` by writing code to compute the best SVM radius which maximizes accuracy on the validation dataset.\n",
    "\n",
    "The provided code will use your `compute_best_svm_radius` to compute and then write the best radius into `output/p06_optimal_radius`.\n",
    "\n",
    "### <font color=red> Answer:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_svm_radius(train_matrix, train_labels, val_matrix, val_labels, radius_to_consider):\n",
    "    \"\"\"Compute the optimal SVM radius using the provided training and evaluation datasets.\n",
    "\n",
    "    You should only consider radius values within the radius_to_consider list.\n",
    "    You should use accuracy as a metric for comparing the different radius values.\n",
    "\n",
    "    Args:\n",
    "        train_matrix: The word counts for the training data\n",
    "        train_labels: The spma or not spam labels for the training data\n",
    "        val_matrix: The word counts for the validation data\n",
    "        val_labels: The spam or not spam labels for the validation data\n",
    "        radius_to_consider: The radius values to consider\n",
    "    \n",
    "    Returns:\n",
    "        The best radius which maximizes SVM accuracy.\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    best_accuracy = 0\n",
    "    best_radius = None\n",
    "    for radius in radius_to_consider:\n",
    "        svm_predict = svm.train_and_predict_svm(train_matrix, train_labels, val_matrix, radius)\n",
    "        current_accuracy = np.mean(svm_predict == val_labels)\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            best_radius = radius\n",
    "    return best_radius\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_messages, train_labels = util.load_spam_dataset('data/ds6_train.tsv')\n",
    "    val_messages, val_labels = util.load_spam_dataset('data/ds6_val.tsv')\n",
    "    test_messages, test_labels = util.load_spam_dataset('data/ds6_test.tsv')\n",
    "    \n",
    "    dictionary = create_dictionary(train_messages)\n",
    "\n",
    "    util.write_json('output/p06_dictionary', dictionary)\n",
    "\n",
    "    train_matrix = transform_text(train_messages, dictionary)\n",
    "\n",
    "    np.savetxt('output/p06_sample_train_matrix', train_matrix[:100,:])\n",
    "\n",
    "    val_matrix = transform_text(val_messages, dictionary)\n",
    "    test_matrix = transform_text(test_messages, dictionary)\n",
    "\n",
    "    naive_bayes_model = fit_naive_bayes_model(train_matrix, train_labels)\n",
    "\n",
    "    naive_bayes_predictions = predict_from_naive_bayes_model(naive_bayes_model, test_matrix)\n",
    "\n",
    "    np.savetxt('output/p06_naive_bayes_predictions.txt', naive_bayes_predictions)\n",
    "\n",
    "    naive_bayes_accuracy = np.mean(naive_bayes_predictions == test_labels)\n",
    "\n",
    "    print('Naive Bayes had an accuracy of {} on the testing set'.format(naive_bayes_accuracy))\n",
    "\n",
    "    top_5_words = get_top_five_naive_bayes_words(naive_bayes_model, dictionary)\n",
    "\n",
    "    print('The top 5 indicative words for Naive Bayes are: ', top_5_words)\n",
    "\n",
    "    util.write_json('output/p06_top_indicative_words', top_5_words)\n",
    "\n",
    "    optimal_radius = compute_best_svm_radius(train_matrix, train_labels, val_matrix, val_labels, [0.01, 0.1, 1, 10])\n",
    "\n",
    "    util.write_json('output/p06_optimal_radius', optimal_radius)\n",
    "\n",
    "    print('The optimal SVM radius was {}'.format(optimal_radius))\n",
    "\n",
    "    svm_predictions = svm.train_and_predict_svm(train_matrix, train_labels, test_matrix, optimal_radius)\n",
    "\n",
    "    svm_accuracy = np.mean(svm_predictions == test_labels)\n",
    "\n",
    "    print('The SVM model had an accuracy of {} on the testing set'.format(svm_accuracy, optimal_radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes had an accuracy of 0.978494623655914 on the testing set\n",
      "The top 5 indicative words for Naive Bayes are:  ['claim', 'won', 'prize', 'tone', 'urgent!']\n",
      "The optimal SVM radius was 0.1\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
