{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Calibration\n",
    "In this question we will try to understand the output $h_{\\theta}(x)$ of the hypothesis function of a logistic regression model, in particular why we might treat the output as a probability (besides the fact that the sigmoid function ensures $h_{\\theta}(x)$ always lies in the interval $(0,1)$).\n",
    "\n",
    "When the probabilities outputted by a model match empirical observation, the model is said to be well calibrated (or reliable). For example, if we consider a set of examples $x^{(i)}$ for which $h_{\\theta}(x^{(i)})\\approx 0.7$, around $70\\%$ of those examples should have positive labels. In a well calibrated model, this property will hold true at every probability value.\n",
    "\n",
    "Logistic regression tends to output well calibrated probabilities (this is often not true with other classifiers such as Naive Bayes, or SVMs). We will dig a little deeper in order to understand why this is the case, and find that the structure of the loss function explains this property.\n",
    "Logistic regression tends to output well calibrated probabilities (this is often not true with other classifiers such as Naive Bayes, or SVMs). We will dig a little deeper in order to understand why this is the case, and find that the structure of the loss function explains this property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a training set $\\{x^{(i)},y^{(i)}\\}^m_{i=1}$ with $x^{(i)}\\in\\mathbb{R}^{n+1}$ and $y^{(i)}\\in \\{0,1\\}$. Assume we have an intercept term $x_0^{(i)} = 1$ for all $i$. Let $\\theta\\in\\mathbb{R}^{n+1}$ be the maximum likelihood parameters\n",
    "learned after training a logistic regression model. In order for the model to be considered well calibrated, given any range of probabilities $(a, b)$ such that $0\\leq a < b \\leq 1$, and training examples $x^{(i)}$ where the model outputs $h_\\theta(x^{(i)})$ fall in the range $(a,b)$, the fraction of positives in that set of examples should be equal to the average of the model outputs for those examples. That is, the following property must hold:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\sum_{i\\in I_{(a,b)}}P\\left(y^{(i)}=1| x^{(i)};\\theta\\right)}{|\\{i\\in I_{(a,b)}\\}|} = \n",
    "\\frac{\\sum_{i\\in I_{(a,b)}}\\mathbb{1}_{y^{(i)}=1}}{|\\{i\\in I_{(a,b)}\\}|}\n",
    "\\end{align*}\n",
    "where \n",
    "\\begin{align*}P(y=1|x;\\theta) = h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^T.x)},\\end{align*}\n",
    "\n",
    "\\begin{align*}I_{(a,b)}= \\left\\{i|i\\in\\{1,\\ldots,m\\}\\}, h_\\theta(x^{(i)})\\in(a,b)\\right\\}\\end{align*}\n",
    "\n",
    "is an index set of all training examples $x^{(i)}$ where $h_\\theta(x^{(i)})$ and $m$ denotes the size of the set $S$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(a)</b> [5 points] Show that the above property holds true for the described logistic regression model over the range $(a, b) = (0, 1)$.\n",
    "\n",
    "<b>Hint:</b> Use the fact that we include a bias term.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the parameter $\\theta\\in \\mathbb{R}^{n+1}$ is the the maximum likelihood parameters learned after training a logistic regression model. Indeed, it is maximize the following log-likelihood objective function:\n",
    "$$l(\\theta) = \\sum_{i=1}^m y^{(i)}\\log h_{\\theta}(x^{(i)}) + \n",
    "(1-y^{(i)})\\log \\left(1-h_{\\theta}(x^{(i)})\\right).$$\n",
    "\n",
    "After training, the obtained parameter $\\theta$ would be a vector making the gradiant of $l(\\theta)$ zero. Lets compute the gradiant of $l(\\theta)$ and set it to zero.\n",
    "$$\\nabla l(\\theta) = \\sum_{i=1}^m \\left(y^{(i)}-h_{\\theta}(x^{(i)})\\right)x^{(i)} = 0.$$\n",
    "\n",
    "Paying attention to the fact that $x_0^{(i)} = 1$ for each $i$, we have \n",
    "$$\\sum_{i=1}^m \\left(y^{(i)}-h_{\\theta}(x^{(i)})\\right) = 0$$\n",
    "and consequently \n",
    "$$\\sum_{i=1}^m y^{(i)} = \\sum_{i=1}^m P\\left(y^{(i)}=1| x^{(i)};\\theta\\right)$$\n",
    "as desired (Note that $I_{(0,1)} = \\{1,\\ldots,m\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(b)</b> [3 points] If we have a binary classification model that is perfectly calibrated—that is, the property we just proved holds for any $(a, b)\\subset [0, 1]$—does this necessarily imply that the model achieves perfect accuracy? Is the converse necessarily true? Justify your answers. \n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has perfect accuracy if and only if \n",
    "\\begin{align*}\n",
    "I_{(0,\\frac{1}{2})}=\\left\\{i: P(y^{(i)} = 1|x^{(i)}; \\theta)< \\frac{1}{2}\\right\\} = \n",
    "\\left\\{i: y^{(i)} = 0\\right\\}.\n",
    "\\end{align*}\n",
    " \n",
    "On the other hand, our model is calibrated for $(0,{1\\over 2})$ is and only if \n",
    "\\begin{align*}\n",
    "\\frac{\\sum_{i\\in I_{(0,\\frac{1}{2})}}P\\left(y^{(i)}=1| x^{(i)};\\theta\\right)}{|\\{i\\in I_{(0,\\frac{1}{2})}\\}|} = \n",
    "\\frac{\\sum_{i\\in I_{(0,\\frac{1}{2})}}\\mathbb{1}_{y^{(i)}=1}}{|\\{i\\in I_{(0,\\frac{1}{2})}\\}|}.\n",
    "\\end{align*}\n",
    "\n",
    "Note that  $0< P(y^{(i)} = 1|x^{(i)}; \\theta) = h_\\theta(x^{(i)}) < \\frac{1}{2}$ for each $i \\in I_{(0,\\frac{1}{2})}$. \n",
    "It simply implies that the two above-mentioned equality cannot hold simultaneously. Therefore, nither a perfectly calibrated model has perfect accuracy nor a model with perfect accuracy is perfectly calibrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(c)</b> [2 points] Discuss what effect including $L_2$ regularization in the logistic regression objective has on model calibration.\n",
    "###  Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation, the objective function becomes \n",
    "$$l(\\theta) = {C\\over 2}\\|\\theta\\|^2_2 + \\sum_{i=1}^m y^{(i)}\\log h_{\\theta}(x^{(i)}) + \n",
    "(1-y^{(i)})\\log \\left(1-h_{\\theta}(x^{(i)})\\right)$$\n",
    "and the, after training, the obtained parameter $\\theta$ would be a vector making the gradiant of $l(\\theta)$ zero. Lets compute the gradiant of $l(\\theta)$ and set it to zero.\n",
    "\n",
    "$$\\nabla l(\\theta) = C\\|\\theta\\| + \\sum_{i=1}^m \\left(y^{(i)}-h_{\\theta}(x^{(i)})\\right)x^{(i)} = 0.$$\n",
    "\n",
    "Paying attention to the fact that $x_0^{(i)} = 1$ for each $i$, we have \n",
    "$$C{\\theta_0} + \\sum_{i=1}^m \\left(y^{(i)}-h_{\\theta}(x^{(i)})\\right) = 0$$\n",
    "and consequently \n",
    "$$C{\\theta_0} + \\sum_{i=1}^m y^{(i)} = \\sum_{i=1}^m P\\left(y^{(i)}=1| x^{(i)};\\theta\\right).$$\n",
    "Therefore, when $\\theta_0\\neq 0$, the model over the range $(a, b) = (0, 1)$ is not calibrated. \n",
    " \n",
    "<b>Remark:</b> If we include $L_2$ regularization $\\|\\theta_{-0}\\|_2^2$, then the obtained model is still calibrated over the range $(a, b) = (0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remark:</b> We considered the range $(a,b) = (0,1)$. This is the only range for which logistic regression is guaranteed to be calibrated on the training set. When the GLM modeling assumptions hold, all ranges $(a, b) \\subset [0, 1]$ are well calibrated. In addition, when the training and test set are from the same distribution and when the model has not overfit or underfit, logistic regression tends to be well calibrated on unseen test data as well. This makes logistic regression a very popular model in practice, especially when we are interested in the level of uncertainty in the model output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
