{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. Neural Networks: MNIST image classification\n",
    "\n",
    "\n",
    "In this problem, you will implement a simple convolutional neural network to classify grayscale images of handwritten digits ($0-9$) from the MNIST dataset. The dataset contains $60,000$ training images and $10,000$ testing images of handwritten digits, $0-9$. Each image is $28\\times 28$ pixels in size with only a single channel. It also includes labels for each example, a number indicating the actual digit ($0-9$) handwritten in that image.\n",
    "\n",
    "The following shows some example images from the [MNIST](https://commons.wikimedia.org/wiki/File:MnistExamples.png) dataset:\n",
    "\n",
    "![Figure 1](Images/Untitled.png)\n",
    "\n",
    "\n",
    "\n",
    "The data for this problem can be found in the data folder as `images_train.csv`, `images_test.csv`, `labels_train.csv` and `labels_test.csv`.\n",
    "\n",
    "The code for this assignment can be found within p1 nn.py within the src folder.\n",
    "\n",
    "The starter code splits the set of $60,000$ training images and labels into a sets of $59,600$ examples\n",
    "as the training set and $400$ examples for dev set.\n",
    "\n",
    "To start, you will implement a simple convolutional neural network and cross entropy loss, and train it with the provided data set.\n",
    "\n",
    "The architecture is as follows:\n",
    "\n",
    "> (a) The first layer is a convolutional layer with $2$ output channels with a convolution size of $4$ by $4$.\n",
    "\n",
    "> (b) The second layer is a max pooling layer of stride and width $5$ by $5$.\n",
    "\n",
    "> (c) The third layer is a ReLU activation layer.\n",
    "\n",
    "> (d) After the four layer, the data is flattened into a single dimension.\n",
    "\n",
    "> (e) The fith layer is a single linear layer with output size $10$ (the number of classes).\n",
    "\n",
    "> (f) The sixth layer is a softmax layer that computes the probabilities for each class.\n",
    "\n",
    "> (g) Finally, we use a cross entropy loss as our loss function.\n",
    "\n",
    "We have provided all of the forward functions for these different layers so there is an unambigious definition of them in the code. Your job in this assignment will be to implement functions that compute the gradients for these layers. However, here is some additional text that might be\n",
    "helpful in understanding the forward functions.\n",
    "\n",
    "We have discussed convolutional layers on the exam, but as a review, the following equation defines what we mean by a $2d$ convolution:\n",
    "\n",
    "\\begin{align*}\n",
    "output[out\\_channel,x,y] \n",
    "& = convolution\\_bias[out\\_channel] + \\\\\n",
    "& \\sum_{di, dj, in\\_channel} input[in\\_channel, x + di, y + dy] \\times convolution\\_weights[out\\_channel, in\\_channel, di, dj]\n",
    "\\end{align*}\n",
    "di and dj iterate through the convolution width and height respectively.\n",
    "\n",
    "The output of a convolution is of size (# output channels, input width - convolution width $ + 1$, output \n",
    "height - convolution height $ + 1$). Note that the dimension of the output is smaller due to padding issues.\n",
    "\n",
    "Max pooling layers simply take the maximum element over a grid. It's defined by the following function\n",
    "\n",
    "\\begin{align*}\n",
    "output[out\\_channel, x, y] = \\max_{di,dj} input[in\\_channel, x \\times pool\\_width + di, y \\times pool\\_height + dy]\n",
    "\\end{align*}\n",
    "\n",
    "The ReLU (rectified linear unit) is our activation function. The ReLU is simply $\\max(0, x)$ where $x$ is the input.\n",
    "We use cross entropy loss as our loss function. Recall that for a single example $(x, y)$, the cross entropy loss is:\n",
    "\n",
    "\\begin{align*}\n",
    "CE(y,\\hat{y}) = -\\sum_{k=1}^K y_k\\log \\hat{y}_k,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\hat{y}_k\\in\\mathbb{R}^k$ is the vector of softmax outputs from the model for the training example $x$, and\n",
    "$y\\in\\mathbb{R}^k$ is theground-truth vector for the training examplex such that $y = [0,\\ldots,0,1,0\\ldots,0]^T$ contains a single $1$ at the position of the correct class (also called a “one-hot” representation).\n",
    "\n",
    "We are also doing mini-batch gradient descent with a batch size of $16$. Normally we would iterater over the data multiple times with multiple epochs, but for this assignment we only do $400$ batches to save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [20 points]\n",
    "Implement the following functions within `p1_nn.py`. We recommend that you start at the top of the list and work your way down:\n",
    "\n",
    "\n",
    "> i. backward softmax \n",
    "\n",
    "> ii. backward relu\n",
    "\n",
    "> iii. backward log loss \n",
    "\n",
    "> iv. backward linear\n",
    "\n",
    "> v. backward convolution \n",
    "\n",
    ">vi. backward max pool\n",
    "\n",
    "\n",
    "### Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "\n",
    "    x = np.reshape(x, (x.shape[0], 1, 28, 28))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = read_data('data/mnist/images_train.csv', 'data/mnist/labels_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POOL_SIZE = 5\n",
    "CONVOLUTION_SIZE = 4\n",
    "CONVOLUTION_FILTERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the output from a cross entropy loss layer given the probabilities and labels.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be a scalar\n",
    "\n",
    "    Returns:\n",
    "        The result of the log loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            result += -np.log(probabilities[i])\n",
    "\n",
    "    return result\n",
    "\n",
    "def backward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross entropy loss with respect to the probabilities.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be the gradient with respect to the probabilities.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    return -labels/probabilities\n",
    "\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for a single example.\n",
    "    The shape of the input is of size # num classes.\n",
    "\n",
    "    Important Note: You must be careful to avoid overflow for this function. Functions\n",
    "    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n",
    "    You will know that your function is overflow resistent when it can handle input like:\n",
    "    np.array([[10000, 10010, 10]]) without issues.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array containing the softmax results of shape  number_of_classes\n",
    "    \"\"\"\n",
    "    x = x - np.max(x,axis=0)\n",
    "    exp = np.exp(x)\n",
    "    s = exp / np.sum(exp,axis=0)\n",
    "    return s\n",
    "\n",
    "def backward_softmax(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x.\n",
    "\n",
    "    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "        grad_outputs: A 1d numpy flaot array of shape number_of_classes (the grad of the loss w.r.t. probabilities)\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n",
    "    \"\"\"\n",
    "    \n",
    "    # *** START CODE HERE ***\n",
    "    z = forward_softmax(x)\n",
    "    d  = z.shape[0]\n",
    "    temp = np.zeros((d,d))\n",
    "    np.fill_diagonal(temp, z)\n",
    "    dx = -z.reshape(-1,1) @ z.reshape(1,-1) + temp\n",
    "    dx = dx @ grad_outputs.reshape(-1,1)\n",
    "    \n",
    "    return dx.reshape(-1,)    \n",
    "    \n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu function for the input x.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy float array\n",
    "\n",
    "    Returns:\n",
    "        A numpy float array containing the relu results\n",
    "    \"\"\"\n",
    "\n",
    "    x[x<=0] = 0\n",
    "\n",
    "    return x\n",
    "\n",
    "def backward_relu(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x\n",
    "\n",
    "    Args:\n",
    "        x: A numpy array of arbitrary shape containing the input.\n",
    "        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n",
    "            to the output of relu\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the same shape as x containing the gradients with respect to x.\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    return grad_outputs * (x>0)\n",
    "    \n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_params():\n",
    "    \"\"\"\n",
    "    Compute the initial parameters for the neural network.\n",
    "\n",
    "    This function should return a dictionary mapping parameter names to numpy arrays containing\n",
    "    the initial values for those parameters.\n",
    "\n",
    "    There should be four parameters for this model:\n",
    "    W1 is the weight matrix for the convolutional layer\n",
    "    b1 is the bias vector for the convolutional layer\n",
    "    W2 is the weight matrix for the output layers\n",
    "    b2 is the bias vector for the output layer\n",
    "\n",
    "    Weight matrices should be initialized with values drawn from a random normal distribution.\n",
    "    The mean of that distribution should be 0.\n",
    "    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that \n",
    "    feed into an output for that layer.\n",
    "\n",
    "    Bias vectors should be initialized with zero.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        A dict mapping parameter names to numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    size_after_convolution = 28 - CONVOLUTION_SIZE + 1\n",
    "    size_after_max_pooling = size_after_convolution // MAX_POOL_SIZE\n",
    "\n",
    "    num_hidden = size_after_max_pooling * size_after_max_pooling * CONVOLUTION_FILTERS\n",
    "\n",
    "    return {\n",
    "        'W1': np.random.normal(size = (CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1/ math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n",
    "        'b1': np.zeros(CONVOLUTION_FILTERS),\n",
    "        'W2': np.random.normal(size = (num_hidden, 10), scale = 1/ math.sqrt(num_hidden)),\n",
    "        'b2': np.zeros(10)\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = np.sum(\n",
    "                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n",
    "\n",
    "\n",
    "    return output\n",
    "\n",
    "def backward_convolution(conv_W, conv_b, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the parameters of the convolution.\n",
    "\n",
    "    See forward_convolution for the sizes of the arguments.\n",
    "    output_grad is the gradient of the loss with respect to the output of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing 3 gradients.\n",
    "        The first element is the gradient of the loss with respect to the convolution weights\n",
    "        The second element is the gradient of the loss with respect to the convolution bias\n",
    "        The third element is the gradient of the loss with respect to the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    \n",
    "    dw = np.zeros(conv_W.shape)\n",
    "    dd = np.zeros(data.shape)\n",
    "    db = np.zeros(conv_channels)\n",
    "    \n",
    "    #for output_channel in range(conv_channels):\n",
    "    #    for x in range(input_width - conv_width + 1):\n",
    "    #        for y in range(input_height - conv_height + 1):\n",
    "                \n",
    "    #            dw[output_channel] += data[:, x:(x + conv_width), y:(y + conv_height)] * output_grad[output_channel,x,y]\n",
    "    #            dd[:, x:(x + conv_width), y:(y + conv_height)] += conv_W[output_channel] * output_grad[output_channel,x,y]\n",
    "    \n",
    "    \n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            \n",
    "            temp1 = data[:, x:(x + conv_width), y:(y + conv_height)].reshape(1, -1)\n",
    "            dw += (temp1 * output_grad[:,x,y].reshape(-1,1)).reshape(dw.shape)\n",
    "            \n",
    "            temp2 = conv_W.reshape(conv_channels,-1) * (output_grad[:,x,y].reshape(-1,1))\n",
    "            temp2 = temp2.sum(axis = 0).reshape(input_channels, conv_width, conv_height)\n",
    "            dd[:, x:(x + conv_width), y:(y + conv_height)] += temp2\n",
    "            \n",
    "    \n",
    "    db = output_grad.sum(axis = (1,2))\n",
    "    \n",
    "    return dw, db, dd\n",
    "\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_max_pool(data, pool_width, pool_height):\n",
    "    \"\"\"\n",
    "    Compute the output from a max pooling layer given the data and pool dimensions.\n",
    "\n",
    "    The stride length should be equal to the pool size\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "\n",
    "    The output should be the result of the max pooling layer and should be of size:\n",
    "        (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    Returns:\n",
    "        The result of the max pooling layer\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((input_channels, input_width // pool_width, input_height // pool_height))\n",
    "\n",
    "    for x in range(0, input_width, pool_width):\n",
    "        for y in range(0, input_height, pool_height):\n",
    "\n",
    "            output[:, x // pool_width, y // pool_height] = np.amax(data[:, x:(x + pool_width), y:(y + pool_height)], axis=(1, 2))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def backward_max_pool(data, pool_width, pool_height, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the data in the max pooling layer.\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of the backward max\n",
    "    pool layer.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the data (of same shape as data)\n",
    "    \"\"\"  \n",
    "    \n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    \n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    grad = np.zeros_like(data)\n",
    "    \n",
    "    for x in range(0, input_width, pool_width):\n",
    "        for y in range(0, input_height, pool_height):\n",
    "            for c in range(input_channels):\n",
    "            \n",
    "                P = data[c, x:(x + pool_width), y:(y + pool_height)]\n",
    "                argmax = np.argmax(P.reshape(-1))\n",
    "                a,b = np.unravel_index(argmax, P.shape)\n",
    "        \n",
    "                grad[c,x+a,y+b] = output_grad[c,x // pool_width, y // pool_height]\n",
    "\n",
    "    \n",
    "    return grad\n",
    "\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(weights, bias, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a linear layer with the given weights, bias and data.\n",
    "    weights is of the shape (input # features, output # features)\n",
    "    bias is of the shape (output # features)\n",
    "    data is of the shape (input # features)\n",
    "\n",
    "    The output should be of the shape (output # features)\n",
    "\n",
    "    Returns:\n",
    "        The result of the linear layer\n",
    "    \"\"\"\n",
    "    return data.dot(weights) + bias\n",
    "\n",
    "\n",
    "def backward_linear(weights, bias, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the loss with respect to the parameters of a linear layer.\n",
    "\n",
    "    See forward_linear for information about the shapes of the variables.\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of this layer.\n",
    "\n",
    "    This should return a tuple with three elements:\n",
    "    - The gradient of the loss with respect to the weights\n",
    "    - The gradient of the loss with respect to the bias\n",
    "    - The gradient of the loss with respect to the data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    dd = weights.dot(output_grad.reshape(-1,))\n",
    "    dw = data.reshape(-1,1)@output_grad.reshape(1,-1)\n",
    "    db = output_grad\n",
    "    \n",
    "    return dw, db, dd\n",
    "    # *** END CODE HERE ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## (b) [10 points] \n",
    "\n",
    "Now implement a function that computes the full backward pass. \n",
    "\n",
    "> i. backward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the forward layer given the data, labels, and params.\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input (shape is 1 by 28 by 28)\n",
    "        labels: A 1d numpy array containing the labels (shape is 10)\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A 2 element tuple containing:\n",
    "            1. A numpy array The output (after the softmax) of the output layer\n",
    "            2. The average loss for these data elements\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "   \n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(y, labels)\n",
    "\n",
    "    return y, cost\n",
    "\n",
    "\n",
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the backward propegation gradient computation step for a neural network\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input for a single example\n",
    "        labels: A 1d numpy array containing the labels for a single example\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2, and b2\n",
    "            W1 and b1 represent the weights and bias for the convolutional layer\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "        \n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    \n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    \n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "   \n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    probs = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(probs, labels)\n",
    "    \n",
    "    \n",
    "    grad_loss_probs = backward_cross_entropy_loss(probs, labels)\n",
    "    \n",
    "    grad_loss_logits = backward_softmax(logits, grad_loss_probs)\n",
    "    \n",
    "    dW2, db2, dz3 = backward_linear(W2, b2, flattened, grad_loss_logits)\n",
    "    \n",
    "    dz2 = backward_relu(first_max_pool, dz3.reshape(first_max_pool.shape))\n",
    "    \n",
    "    dz1 = backward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE, dz2)\n",
    "    \n",
    "    dW1, db1, _ = backward_convolution(W1, b1, data, dz1)\n",
    "    \n",
    "    return {\n",
    "        'W1': dW1,\n",
    "        'b1': db1,\n",
    "        'W2': dW2,\n",
    "        'b2': db2\n",
    "    }\n",
    "    \n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n",
    "    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n",
    "\n",
    "    y_array = []\n",
    "    cost_array = []\n",
    "\n",
    "    for item, label in zip(batch_data, batch_labels):\n",
    "        y, cost = forward_prop_func(item, label, params)\n",
    "        y_array.append(y)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "    return np.array(y_array), np.array(cost_array)\n",
    "\n",
    "def gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n",
    "    \"\"\"\n",
    "    Perform one batch of gradient descent on the given training data using the provided learning rate.\n",
    "\n",
    "    This code should update the parameters stored in params.\n",
    "    It should not return anything\n",
    "\n",
    "    Args:\n",
    "        batch_data: A numpy array containing the training data for the batch\n",
    "        train_labels: A numpy array containing the training labels for the batch\n",
    "        learning_rate: The learning rate\n",
    "        params: A dict of parameter names to parameter values that should be updated.\n",
    "        backward_prop_func: A function that follows the backwards_prop API\n",
    "\n",
    "    Returns: This function returns nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    total_grad = {}\n",
    "\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        grad = backward_prop_func(\n",
    "            batch_data[i, :, :], \n",
    "            batch_labels[i, :], \n",
    "            params)\n",
    "        for key, value in grad.items():\n",
    "            if key not in total_grad:\n",
    "                total_grad[key] = np.zeros(value.shape)\n",
    "\n",
    "            total_grad[key] += value\n",
    "\n",
    "    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n",
    "    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n",
    "    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n",
    "    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(\n",
    "    train_data, train_labels, dev_data, dev_labels, \n",
    "    get_initial_params_func, forward_prop_func, backward_prop_func,\n",
    "    learning_rate=5, batch_size=16, num_batches=400):\n",
    "\n",
    "    m = train_data.shape[0]\n",
    "\n",
    "    params = get_initial_params_func()\n",
    "\n",
    "    cost_dev = []\n",
    "    accuracy_dev = []\n",
    "    for batch in range(num_batches):\n",
    "        if batch % 10 == 0:\n",
    "            print('Currently processing {} / {}'.format(batch, num_batches))\n",
    "\n",
    "        batch_data = train_data[batch * batch_size:(batch + 1) * batch_size, :, :, :]\n",
    "        batch_labels = train_labels[batch * batch_size: (batch + 1) * batch_size, :]\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n",
    "            cost_dev.append(sum(cost) / len(cost))\n",
    "            accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "\n",
    "            print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n",
    "\n",
    "        gradient_descent_batch(batch_data, batch_labels, \n",
    "            learning_rate, params, backward_prop_func)\n",
    "        \n",
    "\n",
    "    output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n",
    "    cost_dev.append(sum(cost) / len(cost))\n",
    "    accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "    print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n",
    "    \n",
    "    return params, cost_dev, accuracy_dev\n",
    "\n",
    "def nn_test(data, labels, params):\n",
    "    output, cost = forward_pr(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    correct_output = np.argmax(output,axis=1)\n",
    "    correct_labels = np.argmax(labels,axis=1)\n",
    "\n",
    "    is_correct = [a == b for a,b in zip(correct_output, correct_labels)]\n",
    "\n",
    "    accuracy = sum(is_correct) * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def run_train(all_data, all_labels, backward_prop_func):\n",
    "    params, cost_dev, accuracy_dev = nn_train(\n",
    "        all_data['train'], all_labels['train'], \n",
    "        all_data['dev'], all_labels['dev'],\n",
    "        get_initial_params, forward_prop, backward_prop_func,\n",
    "        learning_rate=1e-2, batch_size=16, num_batches=400\n",
    "    )\n",
    "\n",
    "    t = np.arange(1 + 400 // 100)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    ax1.plot(t, cost_dev, 'b')\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_title('Training curve')\n",
    "    \n",
    "    ax2.plot(t, accuracy_dev, 'b')\n",
    "    ax2.set_xlabel('time')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "\n",
    "    fig.savefig('output/train.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_data, train_labels):\n",
    "    np.random.seed(100)\n",
    "    #train_data, train_labels = read_data('data/mnist/images_train.csv', 'data/mnist/labels_train.csv')\n",
    "    train_labels = one_hot_labels(train_labels)\n",
    "    p = np.random.permutation(60000)\n",
    "    train_data = train_data[p,:]\n",
    "    train_labels = train_labels[p,:]\n",
    "\n",
    "    dev_data = train_data[0:400,:]\n",
    "    dev_labels = train_labels[0:400,:]\n",
    "    train_data = train_data[400:,:]\n",
    "    train_labels = train_labels[400:,:]\n",
    "\n",
    "    mean = np.mean(train_data)\n",
    "    std = np.std(train_data)\n",
    "    train_data = (train_data - mean) / std\n",
    "    dev_data = (dev_data - mean) / std\n",
    "\n",
    "    all_data = {\n",
    "        'train': train_data,\n",
    "        'dev': dev_data,\n",
    "    }\n",
    "\n",
    "    all_labels = {\n",
    "        'train': train_labels,\n",
    "        'dev': dev_labels,\n",
    "    }\n",
    "    \n",
    "    run_train(all_data, all_labels, backward_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 0 / 400\n",
      "Cost and accuracy 2.721417647426753 0.0725\n",
      "Currently processing 10 / 400\n",
      "Currently processing 20 / 400\n",
      "Currently processing 30 / 400\n",
      "Currently processing 40 / 400\n",
      "Currently processing 50 / 400\n",
      "Currently processing 60 / 400\n",
      "Currently processing 70 / 400\n",
      "Currently processing 80 / 400\n",
      "Currently processing 90 / 400\n",
      "Currently processing 100 / 400\n",
      "Cost and accuracy 0.6413721697623076 0.78\n",
      "Currently processing 110 / 400\n",
      "Currently processing 120 / 400\n",
      "Currently processing 130 / 400\n",
      "Currently processing 140 / 400\n",
      "Currently processing 150 / 400\n",
      "Currently processing 160 / 400\n",
      "Currently processing 170 / 400\n",
      "Currently processing 180 / 400\n",
      "Currently processing 190 / 400\n",
      "Currently processing 200 / 400\n",
      "Cost and accuracy 0.43482822210982586 0.8625\n",
      "Currently processing 210 / 400\n",
      "Currently processing 220 / 400\n",
      "Currently processing 230 / 400\n",
      "Currently processing 240 / 400\n",
      "Currently processing 250 / 400\n",
      "Currently processing 260 / 400\n",
      "Currently processing 270 / 400\n",
      "Currently processing 280 / 400\n",
      "Currently processing 290 / 400\n",
      "Currently processing 300 / 400\n",
      "Cost and accuracy 0.36739952605342974 0.87\n",
      "Currently processing 310 / 400\n",
      "Currently processing 320 / 400\n",
      "Currently processing 330 / 400\n",
      "Currently processing 340 / 400\n",
      "Currently processing 350 / 400\n",
      "Currently processing 360 / 400\n",
      "Currently processing 370 / 400\n",
      "Currently processing 380 / 400\n",
      "Currently processing 390 / 400\n",
      "Cost and accuracy 0.2581653942149925 0.9075\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(train_data, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
