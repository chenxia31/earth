{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4. Independent components analysis\n",
    "\n",
    "While studying Independent Component Analysis (ICA) in class, we made an informal argument about why Gaussian distributed sources will not work. We also mentioned that any other distribution (except Gaussian) for the sources will work for ICA, and hence used the logistic distribution instead. In this problem, we will go deeper into understanding why Gaussian distributed sources are a problem. We will also derive ICA with the Laplace distribution, and apply it to the cocktail party problem.\n",
    "\n",
    "Reintroducing notation, let $s\\in\\mathbb{R}^n$ be source data that is generated from $n$ independent sources. Let $x\\in\\mathbb{R}^n$ be observed data such that $x=As$, where $A\\in\\mathbb{R}^{n\\times n}$ is called the mixing matrix. We assume $A$ is invertible, and $W=A^{-1}$ is called the *unmixing matrix*. So, $s=Wx$. The goal of ICA is to estimate $W$. Similar to the notes, we denote $w_j^T$ to be the $j^{th}$ row of $W$. Note that this implies that the $j^{th}$ source can be reconstructed with $w_j$ and $x$, since $s_j=w_j^tx$. We are given a training set $\\{x^{(1)},\\ldots,x^{(m)}\\}$ for the following sub-questions. Let us denote the entire training set by the design matrix $X\\in\\mathbb{R}^{m\\times n}$ where each example corresponds to a row in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [5 points] Gaussian source\n",
    "For this sub-question, we assume sources are distributed according to a standard normal\n",
    "distribution, i.e $s_j\\sim \\mathcal{N}(0,1), j=1,\\ldots,m$. The likelihood of our unmixing matrix, as described in the notes, is\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(W) = \\sum_{i=1}^m\\left(\\log |W| + \\sum_{j=1}^n\\log g'(w^T_jx^{(i)})\\right),\n",
    "\\end{align*}\n",
    "\n",
    "where $g$ is the cumulative distribution function, and $g'$ is the probability density function of the source distribution (in this sub-question it is a standard normal distribution). Whereas in the notes we derive an update rule to train $W$ iteratively, for the cause of Gaussian distributed sources, we can analytically reason about the resulting $W$.\n",
    "Try to derive a closed form expression for W in terms of $X$ when $g$ is the standard normal CDF. Deduce the relation between $W$ and $X$ in the simplest terms, and highlight the ambiguity (in terms of rotational invariance) in computing $W$.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging \n",
    "$g'(w^T_jx^{(i)}) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}w^T_jx^{(i)}{x^{(i)}}^Tw_j\\right)$ \n",
    "into the formula of $\\ell(W)$, we obtain \n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(W) \n",
    "& = C + \\sum_{i=1}^m\\left(\\log |W| + \\sum_{j=1}^n   -\\frac{1}{2}w^T_jx^{(i)}{x^{(i)}}^Tw_j\\right)\\\\\n",
    "& = C + m \\log |W| -\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^n   w^T_jx^{(i)}{x^{(i)}}^Tw_j\\\\\n",
    "& = C + m \\log |W| -\\frac{1}{2}\\sum_{i=1}^m   Wx^{(i)}(Wx^{(i)})^T\\\\\n",
    "& = C + m \\log |W| -\\frac{1}{2}tr\\left(WX(WX)^T\\right)\\\\\n",
    "& = C + m \\log |W| -\\frac{1}{2}tr\\left(WXX^TW^T\\right).\n",
    "\\end{align*}\n",
    "\n",
    "Computing and setting $\\nabla_W(\\ell)$ to zero, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_W(\\ell) \n",
    "& = m (W^T)^{-1} -WXX^T\\\\\n",
    "& = 0,\n",
    "\\end{align*}\n",
    "\n",
    "which implies \n",
    "\n",
    "\\begin{align*}\n",
    "W^TW = m(XX^T)^{-1}.\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, any $W$ maximizing $\\ell$ satisfies this equality. Let $W$ be such a matrix, $R\\in \\mathbb{R}^{n\\times n}$ an arbitrary orthonormal matrix, and set $W'= RW$. In this situation,\n",
    "\n",
    "\\begin{align*}\n",
    "W'^TW'\n",
    "& = W^TR^TRW\\\\\n",
    "& = W^TW.\n",
    "\\end{align*}\n",
    "\n",
    "Using the fact that $|W'|=|W|$, we obtain \n",
    "\\begin{align*}\n",
    "\\ell(W') \n",
    "& = C + m \\log |W'| -\\frac{1}{2}tr\\left(W'XX^TW'^T\\right)\\\\\n",
    "& = C + m \\log |W| -\\frac{1}{2}tr\\left(W'^TW'XX^T\\right)\\\\\n",
    "& = C + m \\log |W| -\\frac{1}{2}tr\\left(W^TWXX^T\\right)\\\\\n",
    "& = C + m \\log |W| -\\frac{1}{2}tr\\left(WXX^TW^T\\right)\\\\\n",
    "& = \\ell(W)\n",
    "\\end{align*}\n",
    "which means that $W'$ maximizes $\\ell$ as well.\n",
    "Therefore, there is no way to say if $W$ should be use as unmixing matrix or $W'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [10 points] Laplace source.\n",
    "For this sub-question, we assume sources are distributed according to a standard Laplace\n",
    "distribution, i.e $s_i\\sim \\mathcal{L}(0,1)$. The Laplace distribution $\\mathcal{L}(0,1)$ has PDF $\\mathcal{f}_{\\mathcal{L}}(s)=\\frac{1}{2}\\exp(-|s|)$. With this assumption, derive the update rule for a single example in the form\n",
    "\n",
    "\\begin{align*}\n",
    "W := W +\\alpha(\\cdots).\n",
    "\\end{align*}\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plug $g'(s) = \\frac{1}{2}\\exp(-|s|)$ into the $\\ell$ sunction to obtain \n",
    "\\begin{align*}\n",
    "\\ell(W) \n",
    "& = \\sum_{i=1}^m\\left(\\log |W| + \\sum_{j=1}^n\\log g'(w^T_jx^{(i)})\\right)\\\\\n",
    "& = C + \\sum_{i=1}^m\\left(\\log |W| - \\sum_{j=1}^n |w^T_jx^{(i)}|\\right).\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, for a training example $x^{(i)}$, \n",
    "\\begin{align*}\n",
    "\\nabla_W\\ell(W) \n",
    "& = (W^T)^{-1} - {\\rm sgn}(Wx^{(i)})  {x^{(i)}}^T\n",
    "\\end{align*}\n",
    "\n",
    "and the update rule for a single example is \n",
    "\n",
    "\\begin{align*}\n",
    "W := W +\\alpha\\left((W^T)^{-1} - {\\rm sgn}(Wx^{(i)})  {x^{(i)}}^T\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [5 points] Cocktail Party Problem\n",
    "For this question you will implement the Bell and Sejnowski ICA algorithm, but assuming a Laplace source (as derived in part-b), instead of the Logistic distribution covered in class. The file `mix.dat` contains the input data which consists of a matrix with $5$ columns, with each column corresponding to one of the mixed signals $x_i$. The code for this question can be found in `p4_ica.py`.\n",
    "\n",
    "Implement the update W and unmix functions in `p4_ica.py`.\n",
    "\n",
    "You can then run `p4_ica.py` in order to split the mixed audio into its components. The mixed audio tracks are written to `midex_i.wav` in the output folder. The split audio tracks are written to `split_i.wav` in the output folder.\n",
    "\n",
    "To make sure your code is correct, you should listen to the resulting unmixed sources. (Some overlap or noise in the sources may be present, but the different sources should be pretty clearly separated.)\n",
    "\n",
    "__Note:__ In our implementation, we anneal the learning rate $\\alpha$ (slowly decreased it over time) to speed up learning. In addition to using the variable learning rate to speed up convergence, one thing that we also do is choose a random permutation of the training data, and running stochastic gradient ascent visiting the training data in that order (each of the specified learning rates was then used for one full pass through the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (53442, 5)\n",
      "Separating tracks ...\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.02\n",
      "0.02\n",
      "0.01\n",
      "0.01\n",
      "0.005\n",
      "0.005\n",
      "0.002\n",
      "0.002\n",
      "0.001\n",
      "0.001\n",
      "unmixer:  [[ 52.83441795  16.79422341  19.94579171 -10.20014236 -20.89404044]\n",
      " [ -9.90390401  -0.97551781  -4.65143455   8.03957138   1.77059925]\n",
      " [  8.29461991  -7.47270509  19.31979841  15.18218038 -14.34599704]\n",
      " [-14.6653108  -26.64475534   2.44392828  21.37767282  -8.41925368]\n",
      " [ -0.27381437  18.38342182   9.31885365   9.10269483  30.60517381]]\n"
     ]
    }
   ],
   "source": [
    "def update_W(W, x, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform a gradient ascent update on W using data element x and the provided learning rate.\n",
    "\n",
    "    This function should return the updated W.\n",
    "\n",
    "    Use the laplace distribiution in this problem.\n",
    "\n",
    "    Args:\n",
    "        W: The W matrix for ICA\n",
    "        x: A single data element\n",
    "        learning_rate: The learning rate to use\n",
    "\n",
    "    Returns:\n",
    "        The updated W\n",
    "    \"\"\"\n",
    "    \n",
    "    # *** START CODE HERE ***\n",
    "    updated_W = W + learning_rate * (np.linalg.inv(W.T) - np.sign(W@x.reshape(-1,1))@x.reshape(1,-1))\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    return updated_W\n",
    "\n",
    "\n",
    "def unmix(X, W):\n",
    "    \"\"\"\n",
    "    Unmix an X matrix according to W using ICA.\n",
    "\n",
    "    Args:\n",
    "        X: The data matrix\n",
    "        W: The W for ICA\n",
    "\n",
    "    Returns:\n",
    "        A numpy array S containing the split data\n",
    "    \"\"\"\n",
    "\n",
    "    S = np.zeros(X.shape)\n",
    "\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    S =  X @ W.T \n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    return S\n",
    "\n",
    "\n",
    "Fs = 11025\n",
    "\n",
    "def normalize(dat):\n",
    "    return 0.99 * dat / np.max(np.abs(dat))\n",
    "\n",
    "def load_data():\n",
    "    mix = np.loadtxt('data/mix.dat')\n",
    "    return mix\n",
    "\n",
    "def save_sound(audio, name):\n",
    "    scipy.io.wavfile.write('output/{}.wav'.format(name), Fs, audio)\n",
    "\n",
    "def unmixer(X):\n",
    "    M, N = X.shape\n",
    "    W = np.eye(N)\n",
    "\n",
    "    anneal = [0.1 , 0.1, 0.1, 0.05, 0.05, 0.05, 0.02, 0.02, 0.01 , 0.01, 0.005, 0.005, 0.002, 0.002, 0.001, 0.001]\n",
    "    print('Separating tracks ...')\n",
    "    for lr in anneal:\n",
    "        print(lr)\n",
    "        rand = np.random.permutation(range(M))\n",
    "        for i in rand:\n",
    "            x = X[i]\n",
    "            W = update_W(W, x, lr)\n",
    "\n",
    "    return W\n",
    "\n",
    "def main():\n",
    "    X = normalize(load_data())\n",
    "\n",
    "    print('X.shape: ', X.shape)\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        save_sound(X[:, i], 'mixed_{}'.format(i))\n",
    "\n",
    "    W = unmixer(X)\n",
    "    print('unmixer: ', W)\n",
    "    S = normalize(unmix(X, W))\n",
    "\n",
    "    for i in range(S.shape[1]):\n",
    "        save_sound(S[:, i], 'split_{}'.format(i))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
