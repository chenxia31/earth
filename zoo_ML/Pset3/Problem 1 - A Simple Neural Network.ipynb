{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A Simple Neural Network\n",
    "\n",
    "Let $X = \\{x^{(1)}, \\ldots, x^{(m)}\\}$ be a dataset of $m$ samples with $2$ features, i.e $x^{(i)}\\in \\mathbb{R}^2$. The samples are classified into $2$ categories with labels $y^{(i)}\\in \\{0,1\\}$. A scatter plot of the dataset is shown in Figure 1:\n",
    "\n",
    "![Figure 1](Figure1.1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in class 1 are marked as as \"$\\times$\" and examples in class 0 are marked as \"$o$\". We want to perform binary classification using a simple neural network with the architecture shown in Figure 2:\n",
    "![Figure 2](Figure1.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote the two features $x_1$ and $x_2$, the three neurons in the hidden layer $h_1,h_2$, and $h_3$, and\n",
    "the output neuron as $o$. Let the weight from $x_i$ to $h_j$ be $w^{[1]}_{ij}$ for $i\\in\\{1,2\\}, j\\in\\{1,2,3\\}$, and the weight from $h_j$ to $o$ be $w^{[2]}_j$. Finally, denote the intercept weight for $h_j$ as $w^{[1]}_{0j}$ , and the intercept weight for $o$ as $w^{[2]}_{0}$. For the loss function, weâ€™ll use average squared loss instead of the usual negative log-likelihood:\n",
    "\\begin{align*}\n",
    "l = \\frac{1}{m}\\sum_{i=1}^m\\left(o^{(i)} - y^{(i)}\\right)^2\n",
    "\\end{align*}\n",
    "where $o^{(i)}$ is the result of the output neuron for example $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) [5 points] \n",
    "\n",
    "Suppose we use the sigmoid function as the activation function for $h_1,h_2,h_3$ and\n",
    "$o$. What is the gradient descent update to $w_{1,2}^{[1]}$ , assuming we use a learning rate of $\\alpha$? Your answer should be written in terms of $x^{(i)}, o^{(i)}, y^{(i)}$, and the weights.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, for example $i$, \n",
    "\\begin{align*}\n",
    "o^{(i)} =& \\sigma(Z^{(2)}) & \\quad & shape: (1,1)\\\\\n",
    "Z^{(2)} =& W^{[2]}H + W^{[2]}_0 & \\quad & shape: (1,3)\\times (3,1) +(1,1) = (1,1)\\\\\n",
    "H =& \\sigma(Z^{(1)}) & \\quad & shape = (3,1)\\\\\n",
    "Z^{(1)} =& W^{[1]}X^{(i)} + W^{[1]}_0  & \\quad & shape = (3,2)\\times (2,1) +(3,1) = (3,1)\\\\\n",
    "\\end{align*}\n",
    "Therefore, \n",
    "\\begin{align*}\n",
    "\\frac{\\partial o^{(i)}}{\\partial W^{[1]}} =\n",
    "& o^{(i)}\\left(o^{(i)}-1\\right) \\Big({W^{[2]}}^T * H * (H -1)\\Big)^T \\times {X^{(i)}}^T\\\\\n",
    "\\end{align*}\n",
    "and consequently, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial o^{(i)}}{\\partial W^{[1]}_{12}}  \n",
    "&= o^{(i)}\\left(o^{(i)}-1\\right) \\big[\\Big({W^{[2]}}^T * H * (H -1)\\Big)^T \\times {X^{(i)}}^T\\big]_{12}\\\\\n",
    "&= o^{(i)}\\left(o^{(i)}-1\\right) w^{[2]}_{1}h^{(i)}_1(h^{(i)}_1-1)x^{(i)}_2\n",
    "\\end{align*}\n",
    "This implies \n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial w^{[1]}_{12}} = w^{[2]}_{1}\\frac{2}{m}\\sum_{i=1}^m (o^{(i)} - y^{(i)})\n",
    "o^{(i)}\\left(o^{(i)}-1\\right) h^{(i)}_1(h^{(i)}_1-1)x^{(i)}_2.\n",
    "\\end{align*}\n",
    "\n",
    "The gradient descent update rule for $w^{[1]}_{12}$ is \n",
    "\\begin{align*}\n",
    "w^{[1]}_{12} := w^{[1]}_{12} - \\alpha \\Big(w^{[2]}_{1}\\frac{2}{m}\\sum_{i=1}^m (o^{(i)} - y^{(i)})\n",
    "o^{(i)}\\left(o^{(i)}-1\\right) h^{(i)}_1(h^{(i)}_1-1)x^{(i)}_2\\Big).\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) [10 points] \n",
    "\n",
    "Now, suppose instead of using the sigmoid function for the activation function\n",
    "for $h_1,h_2,h_3$ and $o$, we instead used the step function $f(x)$, defined as\n",
    "\n",
    "\\begin{align*}\n",
    "f(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & x\\geq 0\\\\\n",
    "0 & x<0.\n",
    "\\end{array}\\right.\n",
    "\\end{align*}\n",
    "\n",
    "Is it possible to have a set of weights that allow the neural network to classify this dataset with 100% accuracy?\n",
    "\n",
    "If it is possible, please provide a set of weights that enable 100% accuracy by completing `optimal_step_weights` within `src/p01_nn.py` and explain your reasoning for those weights in your PDF.\n",
    "\n",
    "If it is not possible, please explain your reasoning in your PDF. (There is no need to modify optimal step weights if it is not possible.)\n",
    "\n",
    "__Hint:__ There are three sides to a triangle, and there are three neurons in the hidden layer.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is possible. $W^{[1]}X = 0$ can be considered as three decision boundaries, for each of them, one side is positive and the other side is negative. \n",
    "\n",
    "\\begin{align*}\n",
    "w^{[1]}_{11}x_1 + w^{[1]}_{12}x_2 + w^{[1]}_{01} = 0\\\\\n",
    "w^{[1]}_{21}x_1 + w^{[1]}_{22}x_2 + w^{[1]}_{02} = 0\\\\\n",
    "w^{[1]}_{31}x_1 + w^{[1]}_{32}x_2 + w^{[1]}_{03} = 0\\\\ \n",
    "\\end{align*}\n",
    "If we set \n",
    "\\begin{align*}\n",
    "w^{[1]}_{11} = 1 \\quad w^{[1]}_{12} = 0  \\quad w^{[1]}_{01} = -0.5\\\\\n",
    "w^{[1]}_{21} = 0 \\quad w^{[1]}_{22} = 1 \\quad w^{[1]}_{02} = -0.5\\\\\n",
    "w^{[1]}_{31} = 1 \\quad w^{[1]}_{32} = 1 \\quad w^{[1]}_{02} = -4,\n",
    "\\end{align*}\n",
    "then these lines form a triangle. For each point $x$ inside this triangle, we have \n",
    "$h_1 = 1, h_2 = 1$, and $h_3=0$. Also, for each point $x$ outside this triangle, we have \n",
    "$h_1 = 0, h_2 = 0$, and $h_3=1$.\n",
    "Therefore, if we set \n",
    "\\begin{align*}\n",
    "w^{[2]}_{1} = 1 \\quad w^{[2]}_{2} = 1  \\quad  w^{[2]}_{3} = 1  \\quad w^{[2]}_{0} = -2,\n",
    "\\end{align*}\n",
    "then whenever $x$ is inside the triangle, output is $+1$ and otherwise is $0$, as desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_weights():\n",
    "    \"\"\"This is an example function that returns weights.\n",
    "    Use this function as a template for optimal_step_weights and optimal_sigmoid_weights.\n",
    "    You do not need to modify this class for this assignment.\n",
    "    \"\"\"\n",
    "    w = {}\n",
    "\n",
    "    w['hidden_layer_0_1'] = -0.5\n",
    "    w['hidden_layer_1_1'] = 1\n",
    "    w['hidden_layer_2_1'] = 0\n",
    "    w['hidden_layer_0_2'] = -0.5\n",
    "    w['hidden_layer_1_2'] = 0\n",
    "    w['hidden_layer_2_2'] = 1\n",
    "    w['hidden_layer_0_3'] = -4\n",
    "    w['hidden_layer_1_3'] = 1\n",
    "    w['hidden_layer_2_3'] = 1\n",
    "\n",
    "    w['output_layer_0'] = -2\n",
    "    w['output_layer_1'] = 1\n",
    "    w['output_layer_2'] = 1\n",
    "    w['output_layer_3'] = 1\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) [10 points] \n",
    "\n",
    "Let the activation functions for $h_1,h_2,h_3$ be the linear function $f(x)=x$ and the activation function for o be the same step function as before.\n",
    "Is it possible to have a set of weights that allow the neural network to classify this dataset with 100% accuracy?\n",
    "\n",
    "If it is possible, please provide a set of weights that enable 100% accuracy by completing `optimal_linear_weights` within `src/p01_nn.py` and explain your reasoning for those weights in your PDF.\n",
    "\n",
    "If it is not possible, please explain your reasoning in your PDF. (There is no need to modify optimal linear weights if it is not possible.)\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it is impossible. Indeed, when the the activation functions for $h_1,h_2,h_3$ is the linear function $f(x)=x$, the Neural Network is nothing but the one lawer Neural Network, i.e., $ReLU(WX+b)$ which clearly has a linear decision boundary, while we need a non-linear boundary to achieve 100% accuracy.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
