{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bayesian Interpretation of Regularization\n",
    "<b>Background:</b> In Bayesian statistics, almost every quantity is a random variable, which can either be observed or unobserved. For instance, parameters $\\theta$ are generally unobserved random variables, and data $x$ and $y$ are observed random variables. The joint distribution of all the random variables is also called the *model* (e.g., $p(x, y, \\theta)$). Every unknown quantity can be estimated by conditioning the model on all the observed quantities. Such a conditional distribution over the unobserved random variables, conditioned on the observed random variables, is called the *posterior distribution*. For instance $p(\\theta|x,y)$ is the posterior distribution in the machine learning context. A consequence of this approach is that we are required to endow our model parameters, i.e., $p(\\theta)$, with a *prior distribution*. The prior probabilities are to be assigned before we see the data—they capture our prior beliefs of what the model parameters might be before observing any evidence.\n",
    "\n",
    "In the purest Bayesian interpretation, we are required to keep the entire posterior distribution over the parameters all the way until prediction, to come up with the *posterior predictive distribution*, and the final prediction will be the expected value of the posterior predictive distribution. However in most situations, this is computationally very expensive, and we settle for a compromise that is less pure (in the Bayesian sense)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compromise is to estimate a point value of the parameters (instead of the full distribution) which is the mode of the posterior distribution. Estimating the mode of the posterior distribution is also called *maximum a posteriori estimation* (MAP). That is,\n",
    "\n",
    "$$\\theta_{\\rm MAP} ={\\rm arg} \\max_{\\theta} p(\\theta|x,y).$$\n",
    "\n",
    "Compare this to the *maximum likelihood estimation* (MLE) we have seen previously: \n",
    "\n",
    "$$\\theta_{\\rm MLE} ={\\rm arg}\\max_{\\theta}p(y|x,\\theta)$$\n",
    "\n",
    "In this problem, we explore the connection between MAP estimation, and common regularization techniques that are applied with MLE estimation. In particular, you will show how the choice of prior distribution over $\\theta$ (e.g., Gaussian or Laplace prior) is equivalent to different kinds of regularization (e.g., $L_2$, or $L_1$ regularization). To show this, we shall proceed step by step, showing intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(a)</b> [3 points] Show that $\\theta_{\\rm MAP} = {\\rm arg}\\max_{\\theta} p(y|x,\\theta)p(\\theta)$ if we assume that $p(\\theta) = p(\\theta|x)$. The assumption that $p(\\theta) = p(\\theta|x)$ will be valid for models such as linear regression where the input $x$ are not explicitly modeled by $\\theta$. (Note that this means $x$ and $\\theta$ are marginally independent, but not conditionally independent when $y$ is given.)\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayes rule, we have \n",
    "\\begin{align*}\n",
    "p(\\theta|x,y) & = \\frac{p(y|x, \\theta)p|\\theta|x)}{p(y|x)}\\\\\n",
    "& = \\frac{p(y|x, \\theta)p|(\\theta)}{p(y|x)}.\n",
    "\\end{align*}\n",
    "Since $p(y|x)$ is constant w.r.t. $\\theta$, \n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\rm MAP} \n",
    "& ={\\rm arg} \\max_{\\theta} p(\\theta|x,y)\\\\\n",
    "& ={\\rm arg} \\max_{\\theta} \\frac{p(y|x, \\theta)p|\\theta)}{p(y|x)}\\\\\n",
    "& ={\\rm arg} \\max_{\\theta} p(y|x, \\theta)p|(\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(b)</b> [5 points] Recall that $L_2$ regularization penalizes the $L_2$ norm of the parameters while minimizing the loss (i.e., negative log likelihood in case of probabilistic models). Now we will show that MAP estimation with a zero-mean Gaussian prior over $\\theta$, specifically $\\theta \\sim N(0,\\eta^2 I)$, is equivalent to applying $L_2$ regularization with MLE estimation. Specifically, show that\n",
    "$$\\theta_{\\rm MAP} = {\\rm arg}\\min_{\\theta}-\\log p(y|x,\\theta) + \\lambda\\|\\theta\\|_2^2.$$\n",
    "Also, what is the value of $\\lambda$? \n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that   \n",
    "\\begin{align*}\n",
    "\\theta_{\\rm MAP} \n",
    "& = {\\rm arg}\\max_{\\theta} p(\\vec{y}|X, \\theta)p(\\theta)\\\\\n",
    "&= {\\rm arg}\\max_{\\theta} \\frac{1}{(2\\pi)^{n/2}\\eta^n}\\exp\\left(-\\frac{\\theta\\theta^T}{2\\eta^2}\\right)\\prod_{i=1}^mp(y^{(i)}|x^{(i)}, \\theta)\\\\\n",
    "& = {\\rm arg}\\min_{\\theta} -\\log\\left(\\frac{1}{(2\\pi)^{n/2}\\eta^n}\\exp\\left(-\\frac{\\theta\\theta^T}{2\\eta^2}\\right)\\prod_{i=1}^mp(y^{(i)}|x^{(i)}, \\theta)\\right)\\\\\n",
    "& = {\\rm arg}\\min_{\\theta} \\frac{1}{2\\eta^2}\\theta\\theta^T -\\sum_{i=1}^m \\log\\left(p(y^{(i)}|x^{(i)}, \\theta)\\right)\\\\\n",
    "& = {\\rm arg}\\min_{\\theta} \\frac{1}{2\\eta^2}\\|\\theta\\|_2^2 -\\sum_{i=1}^m \\log\\left(p(y^{(i)}|x^{(i)}, \\theta)\\right).\n",
    "\\end{align*}\n",
    "It is clear that $\\lambda = \\frac{1}{2\\eta^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(c)</b> [7 points] Now consider a specific instance, a linear regression model given by $y = \\theta^T x + \\epsilon$ where $\\epsilon ∼ N(0,\\sigma^2)$. Like before, assume a Gaussian prior on this model such that $\\theta \\sim N(0,\\eta^2 I)$. For notation, let $X$ be the design matrix of all the training example inputs where each row vector is one example input, and $\\vec{y}$ be the column vector of all the example outputs.\n",
    "Come up with a closed form expression for $\\theta_{\\rm MAP}$.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y = \\theta^T x + \\epsilon$ where $\\epsilon ∼ N(0,\\sigma^2)$, then $y^{(i)}|x^{(i)}, \\theta\\sim N(\\theta^Tx^{(i)},\\sigma^2).$\n",
    "Therefore, \n",
    "\\begin{align*}\n",
    "\\theta_{\\rm MAP}\n",
    "& = {\\rm arg}\\min_{\\theta} \\frac{1}{2\\eta^2}\\|\\theta\\|_2^2 -\\sum_{i=1}^m \\log\\left(p(y^{(i)}|x^{(i)}, \\theta)\\right)\\\\\n",
    "& = {\\rm arg}\\min_{\\theta} \\frac{1}{2\\eta^2}\\|\\theta\\|_2^2 -\\sum_{i=1}^m \\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\n",
    "\\exp\\left(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{r2\\sigma^2}\\right)\\right)\\\\\n",
    "& = {\\rm arg}\\min_{\\theta} \\frac{1}{2\\eta^2}\\|\\theta\\|_2^2 +\\frac{1}{2\\sigma^2}\\sum_{i=1}^m (y^{(i)}-\\theta^Tx^{(i)})^2\\\\\n",
    "&= {\\rm arg}\\min_{\\theta} \\frac{1}{2\\eta^2}\\|\\theta\\|_2^2 +\\frac{1}{2\\sigma^2} \\|\\vec{y}-X\\theta\\|_2^2\\\\\n",
    "&= {\\rm arg}\\min_{\\theta} \\frac{1}{2\\eta^2}\\|\\theta\\|_2^2 +\\frac{1}{2\\sigma^2} (\\vec{y}-X\\theta)^T(\\vec{y}-X\\theta)\\\\\n",
    "&= {\\rm arg}\\min_{\\theta} \\frac{\\sigma^2}{\\eta^2}\\|\\theta\\|_2^2 +(\\vec{y}-X\\theta)^T(\\vec{y}-X\\theta)\n",
    "\\end{align*}\n",
    "For \n",
    "$$J(\\theta) = \\frac{\\sigma^2}{\\eta^2}\\|\\theta\\|_2^2 +(\\vec{y}-X\\theta)^T(\\vec{y}-X\\theta),$$\n",
    "\n",
    "to find $\\theta_{\\rm MAP}$, we need to find a $\\theta$ minimizing $J(\\theta)$. Since $J(\\theta)$ is convex, such $\\theta$ can be uniquely determined by setting the gradiant of $J(\\theta)$ to zero:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta}J(\\theta) \n",
    "&= \\frac{\\sigma^2}{\\eta^2}\\theta - X^T(\\vec{y}-X\\theta)\\\\\n",
    "&= \\left(\\frac{\\sigma^2}{\\eta^2}I+X^TX\\right)\\theta - X^T\\vec{y}\\\\\n",
    "& = 0\n",
    "\\end{align*}\n",
    "which results in \n",
    "$$\\theta_{\\rm MAP} = \\left(\\frac{\\sigma^2}{\\eta^2}I+X^TX\\right)^{-1} X^T\\vec{y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(d)</b> [5 points] Next, consider the Laplace distribution, whose density is given by \n",
    "\\begin{align*}\n",
    "f_{\\mathcal L}(z|\\mu,b) = \\frac{1}{2b}\\exp\\left({-|z-\\mu|\\over b}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "As before, consider a linear regression model given by $y = x^T.\\theta + \\epsilon$ where $\\epsilon \\sim N(0, \\sigma^2)$. Assume a Laplace prior on this model, where each parameter $\\theta_i$ is marginally independent, and is distributed as $\\theta_i\\sim L(0, b)$.\n",
    "Show that $\\theta_{\\rm MAP}$ in this case is equivalent to the solution of linear regression with $L_1$ regularization, whose loss is specified as\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\theta) =\\|X\\theta -\\vec{Y}\\|_2^2+\\gamma\\|\\theta\\|_1.\n",
    "\\end{align*}\n",
    "\n",
    "Also, what is the value of $\\gamma$?\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Part (b) and (c), we have  \n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\rm MAP} \n",
    "& = {\\rm arg}\\max_{\\theta} p(\\vec{y}|X, \\theta)p(\\theta)\\\\\n",
    "&= {\\rm arg}\\max_{\\theta} \\prod_{i=1}^n \\frac{1}{2b}\\exp\\left(-\\frac{|\\theta_i|}{ b}\\right)\\prod_{i=1}^mp(y^{(i)}|x^{(i)}, \\theta)\\\\\n",
    "& = {\\rm arg}\\min_{\\theta} -\\log\\left(\\prod_{i=1}^n \\frac{1}{2b}\\exp\\left(-\\frac{|\\theta_i|}{b}\\right)\\prod_{i=1}^m\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\n",
    "\\exp\\left(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}\\right)\\right)\\right)\\\\\n",
    "& = {\\rm arg}\\min_{\\theta} \\frac{1}{b}\\sum_{i=1}^n|\\theta_i| + \\sum_{i=1}^m \\left(y^{(i)}-\\theta^Tx^{(i)}\\right)^2\\\\\n",
    "& = {\\rm arg}\\min_{\\theta} \\frac{2\\sigma^2}{b}\\|\\theta\\|_1 + \\|\\vec{y}-X\\theta\\|_2^2.\n",
    "\\end{align*}\n",
    "\n",
    "It is clear that \n",
    "\\begin{align*}\n",
    "J(\\theta)=\\frac{2\\sigma^2}{b}\\|\\theta\\|_1 + \\|\\vec{y}-X\\theta\\|_2^2\n",
    "\\end{align*} \n",
    "and $\\lambda = \\frac{2\\sigma^2}{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: A closed form solution for linear regression problem with $L_1$ regularization does not exist. To optimize this, we use gradient descent with a random initialization and solve it numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remark__: Linear regression with $L_2$ regularization is also commonly called *Ridge regression*, and when $L_1$ regularization is employed, is commonly called *Lasso regression*. These regularizations can be applied to any Generalized Linear models just as above (by replacing $\\log p(y|x,\\theta)$ with the appropriate family likelihood). Regularization techniques of the above type are also called *weight decay*, and *shrinkage*. The Gaussian and Laplace priors encourage the parameter values to be closer to their mean (*i.e.*, zero), which results in the shrinkage effect.\n",
    "\n",
    "__Remark__: Lasso regression (*i.e.*, $L_1$ regularization) is known to result in sparse parameters, where most of the parameter values are zero, with only some of them non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
