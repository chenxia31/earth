{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Convexity of Generalized Linear Models\n",
    "In this question we will explore and show some nice properties of Generalized Linear Models, specifically those related to its use of Exponential Family distributions to model the output.\n",
    "\n",
    "Most commonly, GLMs are trained by using the negative log-likelihood (NLL) as the loss function. This is mathematically equivalent to Maximum Likelihood Estimation (i.e., maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood). In this problem, our goal is to show that the NLL loss of a GLM is a convex function w.r.t the model parameters. As a reminder, this is convenient because a convex function is one for which any local minimum is also a global minimum.\n",
    "\n",
    "To recap, an exponential family distribution is one whose probability density can be represented\n",
    "$$p(y,\\eta)=b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)$$\n",
    "where $\\eta$ is the natural parameter of the distribution. Moreover, in a Generalized Linear Model, $\\eta$ is modeled as $\\theta^Tx$, where $x\\in \\mathbb{R}^n$ is the input features of the example, and $\\theta\\in\\mathbb{R}^n$ is learnable parameters. In order to show that the NLL loss is convex for GLMs, we break down the process into sub-parts, and approach them one at a time. Our approach is to show that the second derivative (i.e., Hessian) of the loss w.r.t the model parameters is Positive Semi-Definite (PSD) at all values of the model parameters. We will also show some nice properties of Exponential Family distributions as intermediate steps.\n",
    "\n",
    "For the sake of convenience we restrict ourselves to the case where $\\eta$ is a scalar. Assume \n",
    "$p(Y|X,\\theta\\sim{\\rm ExponentialFamily}(\\eta)$\n",
    "where $\\eta\\in \\mathbb{R}$ is a scalar, and $T(y) = y$. This makes the exponential family representation take the form\n",
    "$$p(y,\\eta)=b(y)\\exp\\left(\\eta^Ty - a(\\eta)\\right)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(a)</b> [5 points] Derive an expression for the mean of the distribution. Show that \n",
    "$E[Y;\\eta]=\\frac{\\partial}{\\partial \\eta}a(\\eta)$ \n",
    "(note that $E[Y;\\eta] = E[Y| X; \\theta]$ since $\\eta = \\theta^TX$). In other words, show that the mean of an exponential family distribution is the first derivative of the log-partition function with\n",
    "respect to the natural parameter.\n",
    "\n",
    "<b>Hint:</b> Start with observing that \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial\\eta}\\int p(y,\\eta)dy =\\int \\frac{\\partial}{\\partial\\eta} p(y,\\eta)dy\n",
    "\\end{align*}\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caution:  In our solution, we assume that $\\theta$ is an $m\\times n$ matrix, $x$ is a vertor of length $n$, and $T(y)$ is a vector of length $m$. \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{0}\n",
    "& = \\frac{\\partial}{\\partial\\eta}\\int p(y,\\eta)dy\\\\ \n",
    "& = \\int \\frac{\\partial}{\\partial\\eta} p(y,\\eta)dy\\\\\n",
    "& = \\int \\frac{\\partial}{\\partial\\eta} b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)dy \\\\\n",
    "& = \\int\\left(T(y)- \\frac{\\partial}{\\partial\\eta}a(\\eta)\\right)b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)dy\\\\\n",
    "& = \\int T(y)b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)dy - \\frac{\\partial}{\\partial\\eta}a(\\eta)\\int b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)dy\\\\\n",
    "& = E(T(y)|\\eta)-\\frac{\\partial}{\\partial\\eta}a(\\eta).\n",
    "\\end{align*}\n",
    "#### <b>Remark:</b> By integral of a vector, we here mean elementwise integration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(b)</b> [5 points] Next, derive an expression for the variance of the distribution. In particular,\n",
    "show that ${\\rm Var}(Y;\\eta) = \\frac{\\partial^2}{\\partial \\eta^2}a(\\eta)$ (again, note that \n",
    "${\\rm Var}(Y; \\eta) = {\\rm Var}(Y|X; \\theta)$). In other words, show that the variance of an exponential family distribution is the second derivative of the log-partition function w.r.t. the natural parameter.\n",
    "\n",
    "<b>Hint:</b> Building upon the result in the previous sub-problem can simplify the derivation. \n",
    "### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $\\eta$ is a vector of length $n$.\n",
    "Using what we have seen in previous part, we have\n",
    "\\begin{align*}\n",
    "[\\mathbf{0}]_{n\\times n}\n",
    "& = \\frac{\\partial}{\\partial\\eta}\\int\\left(T(y)- \n",
    "    \\frac{\\partial}{\\partial\\eta}a(\\eta)\\right)b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)dy\\\\    \n",
    "& = -\\frac{\\partial^2}{\\partial\\eta^2}a(\\eta)\\int b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)dy + \\int\\Big(T(y)- \n",
    "    \\frac{\\partial}{\\partial\\eta}a(\\eta)\\Big)\\Big(T(y)- \n",
    "    \\frac{\\partial}{\\partial\\eta}a(\\eta)\\Big)^Tb(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)dy\\\\    \n",
    "& = -\\frac{\\partial^2}{\\partial\\eta^2}a(\\eta) + \\int\\Big(T(y)- \n",
    "    E(T(y);\\eta)\\Big)\\Big(T(y)- E(T(y);\\eta)\\Big)^Tb(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)dy\\\\   \n",
    "& = -\\frac{\\partial^2}{\\partial\\eta^2}a(\\eta) + {\\rm Cov}(T(Y);\\eta).\n",
    "\\end{align*}\n",
    "    as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(c)</b> [5 points] Finally, write out the loss function $l(\\theta)$, the NLL of the distribution, as a function of $\\theta$. Then, calculate the Hessian of the loss w.r.t $\\theta$, and show that it is always PSD. This concludes the proof that NLL loss of GLM is convex.\n",
    "\n",
    "<b>Hint 1:</b> Use the chain rule of calculus along with the results of the previous parts to simplify your derivations.\n",
    "\n",
    "<b>Hint 2:</b> Recall that variance of any probability distribution is non-negative. \n",
    "\n",
    "###  Answer: \n",
    "<b>Remark:</b> The main takeaways from this problem are:\n",
    "1. Any GLM model is convex in its model parameters.\n",
    "2. The exponential family of probability distributions are mathematically nice. Whereas calculating mean and variance of distributions in general involves integrals (hard), surprisingly we can calculate them using derivatives (easy) for exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simlicity in notation, assume that $\\theta$ is of shape $(m, n)$.\n",
    "\n",
    "\\begin{align*}\n",
    "l(\\theta) \n",
    "& = -\\log p(y|\\eta)\\\\\n",
    "& = -\\log b(y)\\exp\\left(\\eta^TT(y) - a(\\eta)\\right)\\\\\n",
    "& = C - \\left(\\eta^TT(y) - a(\\eta)\\right)\\\\\n",
    "& = C - \\left(x^T\\theta T(Y) - a(\\theta^T x)\\right)\n",
    "\\end{align*}\n",
    "where $\\eta = \\theta^T x$.\n",
    "Therefore, \n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial\\theta}l(\\theta) \n",
    "& = -x\\left(T(y) - \\frac{\\partial}{\\partial\\eta}a(\\eta)\\right)^T\n",
    "\\end{align*}\n",
    "and thus, \n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial\\theta_{ij}}l(\\theta) \n",
    "& = -x_i\\left(T(y)_j - \\frac{\\partial}{\\partial\\eta_j}a(\\eta)\\right)\n",
    "\\end{align*}\n",
    "Computing the second derivative of $l$, we obtain \n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial\\theta_{kl}}\\Big(\\frac{\\partial}{\\partial\\theta_{ij}}l(\\theta)\\Big)\n",
    "& = x_i\\frac{\\partial}{\\partial\\theta_{kl}}\\left(\\frac{\\partial}{\\partial\\eta_j}a(\\eta)\\right)\\\\\n",
    "& = x_ix_{k}\\frac{\\partial}{\\partial\\eta_{l}}\\left(\\frac{\\partial}{\\partial\\eta_j}a(\\eta)\\right)\\\\\n",
    "& = x_ix_{k}{\\rm Cov}(T(Y);\\eta)_{jl}\n",
    "\\end{align*}\n",
    "\n",
    "Now, let $Z$ be a matrix with of shape $(m,n)$ (same as $\\theta$) but consider $Z$ as a vector with length $mn$\n",
    "whose coordinates are indexed by $ij$'s. \n",
    "Indeed, the Hessian matrix $H = \\frac{\\partial^2}{\\partial\\theta^2}l(\\theta)$ is of shape $(mn,mn)$ and $Z$ is of vercor of $mn$ elements.  \n",
    "\n",
    "\\begin{align*}\n",
    "\\Big(Z^THZ\\Big)_{ij,kl}\n",
    "& = \\sum_{ij}\\sum_{kl}z_{ij}H_{ij,kl}z_{kl}\\\\\n",
    "& = \\sum_{ij}\\sum_{kl}z_{ij}\\Big(x_ix_{k}{\\rm Cov}(T(Y);\\eta)_{jl}\\Big)z_{kl}\\\\\n",
    "& = \\sum_{ij}\\sum_{kl}x_iz_{ij}\\Big({\\rm Cov}(T(Y);\\eta)_{jl}\\Big)x_{k}z_{kl}\\\\\n",
    "& = x^TZ\\ C\\ Z^Tx\\\\\n",
    "& = (x^TZ)\\ C\\ (x^TZ)^T \\geq 0\n",
    "\\end{align*}\n",
    "since covariance matrix $C = {\\rm Cov}(T(Y);\\eta)$ is positive semi-definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
